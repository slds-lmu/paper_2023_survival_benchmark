---
title: "Resource Estimates"
author: "Lukas"
date: now
date-format: "YYYY-MM-DD HH:mm:ss (z)"
format: 
  html:
    code-fold: true
    toc: true
    toc-depth: 3
editor: source
editor_options: 
  chunk_output_type: console
---

```{r setup}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Setup

Benchmark setup for resource estimation:

- 1 fold of a 3-fold CV for each learner x task combination
- Only run jobs tuned on `harrell_c` and untuned learners (plus CoxBoost)
- Budget:
  - 5 + n_params evaluations for MBO
  - 3-fold CV for tuning (1 iter)

- Extrapolating outer eval should be straight-forward (times 3 for CV, times `n_repeats` as needed)
- Extrapolating tuning budget roughly



```{r setup2}
library(batchtools)
library(ggplot2)
library(data.table)

reg = loadRegistry(conf$reg_dir, writeable = FALSE, work.dir = ".")
tab = unwrap(getJobTable(reg = reg))
setnames(tab, "tags", "measure")

# Enhance with task metadata (n, p, number of unique time points etc)
tasktab = load_tasktab()
lrntab = load_lrntab()

tab = tasktab[tab, on = "task_id"]
tab = lrntab[tab, on = c("id" = "learner_id")]
setnames(tab, "id", "learner_id")

# Estimating runtime via `batchtools` builtin `ranger` only on selected variables (avoid time.queued, repl, note that job.id is excluded automatically):
bt_est = tab |>
  dplyr::select(job.id, task_id, learner_id, measure) |>
  batchtools::estimateRuntimes()
print(bt_est, n = 500)

# Merging estimates runtimes with observed ones, converting to `hms` for readability:
tab = tab[bt_est$runtimes, on = "job.id"]
tab[,
  runtime_hms := hms::hms(seconds = as.numeric(runtime, units = "secs"))
]

scores = readRDS(fs::path(conf$result_path, "scores", ext = "rds"))
scores[, .(n = .N), by = .(learner_id, task_id)]

```

```{r helper_funs}
summarize_resources <- function(xdf, by = "task_id", measure = "runtime") {
  checkmate::assert_subset(by, choices = c("task_id", "learner_id", "measure"))
  checkmate::assert_subset(measure, choices = c("runtime", "mem.used"))

  xtab <- xdf |>
    dplyr::filter(type == "observed") |>
    dplyr::summarise(
      min = min(.data[[measure]]),
      q25 = quantile(.data[[measure]], prob = 0.25),
      median = median(.data[[measure]]),
      q75 = quantile(.data[[measure]], prob = 0.75),
      max = max(.data[[measure]]),
      n_jobs = dplyr::n(),
      .by = dplyr::all_of(by)
    )

  if (measure == "runtime") {
    xtab <- xtab |>
      dplyr::mutate(dplyr::across(
        min:max,
        \(x) hms::hms(seconds = round(x, 1))
      ))
  }
  if (measure == "mem.used") {
    # mem.used shows MB, pretty_bytes expects bytes
    xtab <- xtab |>
      dplyr::mutate(dplyr::across(min:max, \(x) round(x / 1000, 2)))
  }

  xtab |>
    dplyr::arrange(dplyr::desc(.data[["q75"]])) |>
    reactable::reactable(
      striped = TRUE,
      filterable = TRUE,
      defaultPageSize = 20
    )
}

plot_resources <- function(xdf, by = "task_id", measure = "runtime", ...) {
  checkmate::assert_subset(by, choices = c("task_id", "learner_id", "measure"))
  checkmate::assert_subset(measure, choices = c("runtime", "mem.used"))

  p <- xdf |>
    dplyr::filter(..., type == "observed") |>
    ggplot(aes(
      y = reorder(.data[[by]], .data[[measure]]),
      x = .data[[measure]]
    )) +
    geom_boxplot()

  if (measure == "runtime") {
    p <- p + scale_x_time(breaks = scales::pretty_breaks())
  }
  if (measure == "mem.used") {
    p <- p +
      scale_x_continuous(
        breaks = scales::pretty_breaks(),
        labels = \(x) prettyunits::pretty_bytes(x * 1000^2)
      )
  }

  title <- switch(measure, runtime = "Runtime", mem.used = "Memory")
  x_ax <- switch(
    measure,
    runtime = "Runtime (HH:MM:SS)",
    mem.used = "Memory (GiB)"
  )
  p +
    labs(
      title = sprintf("Job %s", title),
      subtitle = sprintf(
        "based on %i / %i jobs (%s%%)",
        sum(tab$type == "observed"),
        nrow(tab),
        round(100 * sum(tab$type == "observed") / nrow(tab), 1)
      ),
      x = x_ax,
      y = by
    ) +
    theme_minimal() +
    theme(legend.position = "top")
}
```

### Learners

Special cases for learners:

- `internal_cv` learners like `glmnet` and `CoxBoost` internally use a cross validation. CB is not tuned on the inner folds at all and uses its own method.
- `encode`/`scale` learners need factor encoding or scaling as part of the preprocessing pipeline
- `grid` learners have such small tuning spaces that a grid search is preferable

```{r show-lrns}
lrntab |>
  dplyr::mutate(dplyr::across(
    dplyr::where(is.logical),
    \(x) ifelse(x, "\u2705", "")
  )) |>
  kableExtra::kbl() |>
  kableExtra::kable_styling()
```

### Tasks

```{r show-tasks}
tasktab |>
  dplyr::select(task_id, n, p, n_uniq_t) |>
  dplyr::arrange(-n) |>
  kableExtra::kbl() |>
  kableExtra::kable_styling()
```

## Status

### How many jobs are done?

```{r show-status}
tab |>
  dplyr::group_by(measure) |>
  dplyr::summarize(
    total = dplyr::n(),
    done = sum(type == "observed"),
    perc = round(100 * done / total, 2)
  ) |>
  kableExtra::kbl() |>
  kableExtra::kable_styling()
```

### Which are still running or expired?

Duration measured in **days** based on "now": `r format(Sys.time(), tz = "Europe/Berlin", format = "%F %T (%Z)")`. If that time is greater than 7 days, they hit the cluster timeout wall.

```{r show-missing, eval = nrow(findRunning()) > 0}
tab[ijoin(findSubmitted(), findNotDone())] |>
  dplyr::mutate(
    duration = round(difftime(Sys.time(), submitted, unit = "days"), 2)
  ) |>
  dplyr::filter(!is.na(duration)) |>
  dplyr::select(-submitted) |>
  reactable::reactable(striped = TRUE, filterable = TRUE)
```


## Runtime 

Only showing observed, not estimated runtimes.

### All Jobs by Runtime
 
```{r}
runtimes <- tab |>
  dplyr::filter(type == "observed") |>
  dplyr::select(
    learner_id,
    task_id,
    n,
    p,
    n_uniq_t,
    measure,
    runtime,
    runtime_hms
  ) |>
  dplyr::arrange(dplyr::desc(runtime))

runtimes |>
  dplyr::mutate(
    runtime_hms = hms::round_hms(runtime_hms, secs = 1)
  ) |>
  dplyr::select(-runtime) |>
  reactable::reactable(striped = TRUE, filterable = TRUE)
```


### Summarizing by learner / task

Limited to finished jobs only, `n_jobs` shows number of jobs used for summary.

Sorted by 75% quantile, descending.

### Runtime per Task

```{r runtime-per-task}
#| column: page

plot_resources(tab, by = "task_id", measure = "runtime")
summarize_resources(tab, by = "task_id", measure = "runtime")
```

### Runtime per Learner

```{r runtime-per-learner}
#| column: page

plot_resources(tab, by = "learner_id", measure = "runtime")
summarize_resources(tab, by = "learner_id", measure = "runtime")
```

### Runtime per Measure

```{r runtime-per-measure}
#| column: page

plot_resources(tab, by = "measure", measure = "runtime")
summarize_resources(tab, by = "measure", measure = "runtime")
```

### Train/Predict

Time converted to hours

```{r train-predict-times}
aggr = readRDS(fs::path(conf$result_path, "aggr.rds"))

aggr |>
  dplyr::select(
    learner_id,
    task_id,
    warnings,
    errors,
    time_train,
    time_predict
  ) |>
  dplyr::mutate(dplyr::across(
    dplyr::starts_with("time"),
    \(x) round(x / 60 / 60, 1)
  )) |>
  reactable::reactable(striped = TRUE, filterable = TRUE, defaultPageSize = 20)

```


## Memory

Raw values of `mem.used` show MB, also converted to GB.

### All jobs by Memory

```{r memory-all}
tab |>
  dplyr::filter(!is.na(mem.used)) |>
  dplyr::select(
    learner_id,
    task_id,
    n,
    p,
    n_uniq_t,
    measure,
    mem.used
  ) |>
  dplyr::arrange(dplyr::desc(mem.used)) |>
  dplyr::mutate(mem.used.gb = round(mem.used / 1000, 3)) |>
  reactable::reactable(striped = TRUE, filterable = TRUE)
```


### Memory per Task

```{r memory-per-task}
#| column: page

plot_resources(tab, by = "task_id", measure = "mem.used")
summarize_resources(tab, by = "task_id", measure = "mem.used")
```

### Memory per Learner

```{r memory-per-learner}
#| column: page

plot_resources(tab, by = "learner_id", measure = "mem.used")
summarize_resources(tab, by = "learner_id", measure = "mem.used")
```

### Memory per Measure

```{r memory-per-measure}
#| column: page

plot_resources(tab, by = "measure", measure = "mem.used")
summarize_resources(tab, by = "measure", measure = "mem.used")
```

## Total Runtime Estimate

Assumptions:

- Here `runtime` is measured on 1 repl of 1 outer resampling fold
- Budget is `tuning_params * 50` evals random search
- Outer folds: 3, but repeats set to 10, 5, or 1, depending on number of events
- Inner folds: 3
- Some learners are not tuned with inner resampling (KM, CoxBoost (internal CV), ...) 
  -> no params, no tuning
  -> no inner CV in that case

Estimated total runtime is then `runtime * outer_folds * inner_folds * n_params * 50`.

Estimates are partially incomplete as some pretest jobs are not completed yet, hence very long runtimes are missing.

```{r runtime-total-sum}
#| column: page
total_estimate <- runtimes |>
  dplyr::group_by(learner_id, task_id, measure) |>
  # Summarize only necessary if more than one repl was submitted pr learner/task/measure
  dplyr::summarize(
    # seconds per 5 evals
    hours = sum(runtime / 60 / 60 / 5),
    .groups = "drop"
  ) |>
  dplyr::left_join(lrntab, by = c("learner_id" = "id")) |>
  dplyr::left_join(tasktab[, c("task_id", "events")], by = c("task_id")) |>
  dplyr::mutate(
    # we tune learners with at least 1 param
    tune = as.integer(params > 0),
    internal_cv = as.integer(internal_cv == "Yes")
  ) |>
  dplyr::mutate(
    n_outer = 3,
    n_outer_repeats = assign_repeats(events),
    # No inner resampling if no tuning
    n_inner = pmax(3 * tune, 1),
    #n_budget = pmax(50 * params * tune, 1),
    n_budget = dplyr::case_when(
      learner_id == "RRT" ~ 46,
      learner_id == "Par" ~ 3,
      learner_id == "Flex" ~ 10,
      .default = pmax(50 * params * tune, 1)
    ),
    total_per_job_raw = hours * n_inner * n_budget,
    total_per_job = pmin(total_per_job_raw, 24 * 7),
    total_h = total_per_job * n_outer * n_outer_repeats,
    total_years = (total_per_job * n_outer * n_outer_repeats) / 24 / 365,
    total_years_raw = (total_per_job_raw * n_outer * n_outer_repeats) / 24 / 365
  ) |>
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \(x) round(x, 2))) |>
  dplyr::select(
    learner_id, task_id, measure, params, tune, events, n_outer, n_outer_repeats,
    total_per_job_raw, total_per_job, total_h, total_years, total_years_raw
  )

total_estimate |>
  dplyr::arrange(-total_h) |>
  reactable::reactable(striped = TRUE, filterable = TRUE)

# total_estimate |>
#   ggplot(aes(x = learner_id, y = total_per_job)) +
#   geom_boxplot() +
#   geom_hline(yintercept = 168, color = "red") +
#   coord_flip()
#
# total_estimate |>
#   ggplot(aes(x = task_id, y = total_per_job)) +
#   geom_boxplot() +
#   geom_hline(yintercept = 168, color = "red") +
#   coord_flip()
```

### Totals per learner:

```{r totals-per-learner}
total_estimate |>
  dplyr::group_by(learner_id) |>
  dplyr::summarise(
    total_h = round(sum(total_h), 2),
    total_years = round(sum(total_years), 2)
  ) |>
  dplyr::arrange(-total_h) |>
  reactable::reactable(striped = TRUE, defaultPageSize = 20)
```

### Totals per task:

```{r totals-per-task}
total_estimate |>
  dplyr::group_by(task_id) |>
  dplyr::summarise(
    total_h = round(sum(total_h), 2),
    total_years = round(sum(total_years), 2)
  ) |>
  dplyr::arrange(-total_h) |>
  reactable::reactable(striped = TRUE, defaultPageSize = 20)
```

### Totals per measure:

```{r totals-per-measure}
total_estimate |>
  dplyr::group_by(measure) |>
  dplyr::summarise(
    total_h = round(sum(total_h), 2),
    total_years = round(sum(total_years), 2)
  ) |>
  dplyr::arrange(-total_h) |>
  reactable::reactable(striped = TRUE, defaultPageSize = 20)
```

### Sum total:

```{r totals-sum}
total_estimate |>
  dplyr::summarise(
    total_h = round(sum(total_h), 2),
    total_years = round(sum(total_years), 2)
  ) |>
  kableExtra::kbl()
```


## Overview

```{r overview}
memtab = tab |>
  dplyr::mutate(
    mem.used = ifelse(is.na(mem.used), max(mem.used, na.rm = TRUE), mem.used)
  ) |>
  dplyr::group_by(learner_id, task_id) |>
  dplyr::summarise(est_mem_mb = mean(mem.used), .groups = "drop")

resource_est_tab = total_estimate |>
  dplyr::left_join(memtab, by = c("learner_id", "task_id")) |>
  dplyr::mutate(
    est_total_hours = ifelse(is.na(total_per_job), max(total_per_job, na.rm = TRUE), total_per_job),
    est_total_hours_raw = ifelse(is.na(total_per_job_raw), max(total_per_job_raw, na.rm = TRUE), total_per_job_raw),
    est_mem_mb = ifelse(is.na(est_mem_mb), max(est_mem_mb, na.rm = TRUE), est_mem_mb)
  ) |>
  dplyr::select(learner_id, task_id, measure, est_total_hours, est_total_hours_raw, est_mem_mb) 

readr::write_csv(
  resource_est_tab,
  file = here::here("tables", "resource_estimates.csv")
)
```
