---
format:
  html:
    page-layout: full
    tbl-cap-location: top
editor: source
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source(here::here("site", "common.R"))

library(reactable)
library(htmltools)
```

This page gives an overview of the benchmark results, including scores aggregated across outer resampling iterations used for later statistical analysis and individual scores per dataset and model.

In general, results are divided by underlying tuning measure, i.e. `harrel_c` and `isbs`, with the former being a measure of discrimination and the latter a proper scoring rule.


## Errros and elapsed time limits

The following table lists the number of errors in the outer resampling iterations per tuning measure (`tune_measure`).
These errors were caused by the learner exceeding the time limit or exceeding memory limitations.
We attempted to resubmit failing computational jobs with increased memory limits, yet in some cases the jobs still failed, at which point we considered the learner/task combination to be computationally infeasible.

We note:

- the affected learners were particularly slow or memory intensive for large tasks with many observations or a large number of unique time points, where the latter in particular appeared even more relevant than the number of observations.
- the tasks below are most often those with many observations and unique time points (hdfail, child, check_times).

We therefore consider the errors to be a result of the learners' complexity and the tasks' size, given reasonable computational constraints.

<details>
<summary>Click to view table</summary>


```{r error-tables-aggr}
err_tbl <- scores |>
  dplyr::group_by(learner_id, task_id, tune_measure) |>
  dplyr::summarise(
    affected_iterations = sum(errors_cnt > 0),
    total_iterations = dplyr::n(),
    .groups = "drop"
  ) |>
  dplyr::filter(affected_iterations > 0) |>
  dplyr::mutate(
    error_rate = round(100 * affected_iterations / total_iterations, 1),
    errors_fmt = glue::glue("{affected_iterations} / {total_iterations} ({error_rate}%)")
  ) |>
  tidyr::pivot_wider(
    id_cols = c("learner_id", "task_id"),
    names_from = "tune_measure",
    values_from = "errors_fmt",
    values_fill = "—"
  )

err_tbl |>
  dplyr::select(-learner_id) |>
  kableExtra::kbl(
    col.names = c("Dataset", "Harrell's C", "ISBS"),
    caption = "Number of evaluations with errors of the total outer resampling iterations by tuning measure. (—) indicates there were no errors during evaluation, but possible during tuning."
  ) |>
  kableExtra::kable_styling() |>
  kableExtra::pack_rows(index = table(err_tbl$learner_id))

```

</details>


## Aggregated Results

Averaged scores across outer resampling folds for each task and learner.

### Boxplots

::: {.panel-tabset}

#### Harrell's C

```{r aggr-boxplot-harrell-c}
#| fig-width: 11
#| fig-height: 8

for (measure_id in msr_tbl[type == "Discrimination", id]) {
  plot_aggr_scores(
    aggr_scores,
    type = "box",
    eval_measure_id = measure_id,
    tuning_measure_id = "harrell_c",
    dodge = FALSE,
    flip = TRUE
  )
}
```

Harrell's C (Scaled)

```{r aggr-boxplot-harrell-c-scaled}
#| fig-width: 11
#| fig-height: 8

for (measure_id in msr_tbl[type == "Discrimination", id]) {
  plot_aggr_scores(
    aggr_scores_scaled,
    type = "box",
    eval_measure_id = measure_id,
    tuning_measure_id = "harrell_c",
    dodge = FALSE,
    flip = TRUE
  ) %+%
    labs(
      title = glue::glue("{msr_tbl[id == measure_id, label]} [Scaled KM – Best]"),
      subtitle = "Boxplot of aggregated scores across all tasks scaled such that 0 = KM, 1 = Best model"
    )
}
```

#### ISBS

```{r aggr-boxplot-isbs}
#| fig-width: 11
#| fig-height: 8

for (measure_id in msr_tbl[type == "Scoring Rule" & !erv, id]) {
  plot_aggr_scores(
    aggr_scores,
    type = "box",
    eval_measure_id = measure_id,
    tuning_measure_id = "isbs",
    dodge = FALSE,
    flip = TRUE
  )
}
```

ISBS (ERV)

```{r aggr-boxplot-isbs-erv}
#| fig-width: 11
#| fig-height: 8

for (measure_id in msr_tbl[type == "Scoring Rule" & erv, id]) {
  plot_aggr_scores(
    aggr_scores,
    type = "box",
    eval_measure_id = measure_id,
    tuning_measure_id = "isbs",
    dodge = FALSE,
    flip = TRUE
  )
}

```

Scaled ISBS

```{r aggr-boxplot-isbs-scaled}
#| fig-width: 11
#| fig-height: 8

for (measure_id in msr_tbl[type == "Scoring Rule" & !erv, id]) {
  plot_aggr_scores(
    aggr_scores_scaled,
    type = "box",
    eval_measure_id = measure_id,
    tuning_measure_id = "isbs",
    dodge = FALSE,
    flip = TRUE
  ) %+%
    labs(
      title = glue::glue("{msr_tbl[id == measure_id, label]} [Scaled KM-Best]"),
      subtitle = "Boxplot of aggregated scores across all tasks scaled such that 0 = KM, 1 = Best model"
    )
}
```

:::

## Results per Dataset

Taking scores from the outer evaluation folds, see `scores.[csv|rds]`.

### Boxplots

::: {.panel-tabset}

#### Harrell's C

```{r scores-boxplot-harrell-c}
#| fig-width: 11
#| fig-height: 11

for (measure_id in msr_tbl[type == "Discrimination", id]) {
  plot_scores(scores, eval_measure_id = measure_id, tuning_measure_id = "harrell_c", dodge = FALSE, flip = TRUE)
}
```

#### ISBS

```{r scores-boxplot-isbs}
#| fig-width: 11
#| fig-height: 11

for (measure_id in msr_tbl[type == "Scoring Rule" & !erv, id]) {
  plot_scores(scores, eval_measure_id = measure_id, tuning_measure_id = "isbs", dodge = FALSE, flip = TRUE)
}
```

:::


### Calibration

#### D-Calibration

Calculating p-values for D-Calibration as `pchisq(score, 10 - 1, lower.tail = FALSE)`.

This represents more of a heuristic approach as an insignificant result implies a well-calibrated model, but a significant result does not necessarily imply a poorly calibrated model.
Furthermore, there is no multiplicity correction applied due to the generally exploratory nature of the plots.

```{r calibration-dcalib-heatmap, fig.width=10, fig.height=8}

aggr_scores |>
  dplyr::filter(grepl("isbs", tune_measure)) |>
  dplyr::mutate(
    dcalib_p = pchisq(dcalib, 10 - 1, lower.tail = FALSE),
    dcalib_label = fifelse(dcalib_p < 0.05, "X", "")
  ) |>
  ggplot(aes(
    x = forcats::fct_reorder(learner_id, dcalib_p),
    y = forcats::fct_rev(task_id),
    fill = dcalib_p
  )) +
  geom_tile(color = "#EEEEEE") +
  geom_text(aes(label = dcalib_label), color = "white", size = 3) +
  # scale_fill_manual(values = c(`TRUE` = "red", `FALSE` = "blue"), labels = c(`TRUE` = "Signif.", `FALSE` = "Not Signif.")) +
  scale_fill_viridis_c(breaks = seq(0, 1, .1)) +
  guides(
    x = guide_axis(n.dodge = 2),
    fill = guide_colorbar(
      title.vjust = .8,
      barwidth = unit(200, "pt")
    )
  ) +
  labs(
    title = "D-Calibration p-values by task and learner",
    subtitle = glue::glue(
      "Models tuned on {msr_tbl[id == 'isbs', label]}\n",
      "Learners ordered by average p-value. X denotes p < 0.05"
    ),
    y = "Task",
    x = "Learner",
    color = NULL,
    fill = "p-value"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title.position = "plot",
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.spacing.x = unit(5, "mm"),
    panel.background = element_rect(fill = "#EEEEEE", color = "#EEEEEE")
  )


```

#### Alpha-Calibration

For this measure, calibration is indicated by a score close to 1.

```{r calibration-alpha-ratio}
ggplot(aggr_scores[grepl("isbs", tune_measure)], aes(y = forcats::fct_rev(learner_id), x = alpha_calib)) +
  geom_point() +
  geom_vline(xintercept = 1) +
  scale_x_log10() +
  labs(
    title = "Alpha-Calibration by task and learner",
    subtitle = glue::glue(
      "Models tuned on {msr_tbl[id == 'isbs', label]}\n",
      "Values close to 1 indicate reasonable calibration"
    ),
    y = "Learner",
    x = "Alpha (log10)"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title.position = "plot",
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank()
    # panel.spacing.x = unit(5, "mm"),
    # panel.background = element_rect(fill = "#EEEEEE", color = "#EEEEEE")
  )

```

### Raw scores

| Variable              | Type  | Description |
| ----------------------|-------|-------------|
| `task_id`             | `fct` | Dataset name, e.g. `veteran`                  |
| `learner_id`          | `fct` | Model / learner name, e.g. `RAN` for `ranger` |
| `harrell_c`           | `dbl` | Evaluation measure score |
| `uno_c`               | `dbl` | Evaluation measure score |
| `isll`                | `dbl` | Evaluation measure score |
| `isll_erv`            | `dbl` | Evaluation measure score |
| `isbs`                | `dbl` | Evaluation measure score |
| `isbs_erv`            | `dbl` | Evaluation measure score |
| `dcalib`              | `dbl` | Evaluation measure score |
| `alpha_calib`         | `dbl` | Evaluation measure score |
| `tune_measure`        | `chr` | Tuning measure, one of `harrell_c`, `isbs` |
| `learner_group`       | `fct` | Model / learner group, one of "Baseline", "Classical"m "Trees", "Boosting" |


```{r aggr-reactable}
#| column: page
#|
aggr_scores |>
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \(x) round(x, 3))) |>
  dplyr::arrange(task_id, learner_id) |>
  reactable::reactable(
    sortable = TRUE,
    filterable = TRUE,
    searchable = TRUE,
    defaultPageSize = 30
  )
```


```{r aggr-raw, eval=FALSE, include=FALSE}
#| column: body-outset
#| results: asis

table_aggr = function(aggr_scores, eval_measure_id, tuning_measure_id) {
  checkmate::assert_data_table(aggr_scores)

  tab = aggr_scores[tuned == tuning_measure_id, c("task_id", "learner_id", ..eval_measure_id)] |>
    group_by(task_id) |>
    mutate(score := round(.data[[eval_measure_id]], 2)) |>
    select(task_id, learner_id, score) |>
    tidyr::pivot_wider(names_from = "learner_id", values_from = "score") |>
    reactable::reactable(
      sortable = TRUE,
      pagination = FALSE,
      filterable = TRUE,
      striped = TRUE
    )

  div(class = "cran-packages", h2(class = "title", "Table title"), tab)
}

table_aggr(aggr_scores, eval_measure_id = "isbs", tuning_measure_id = "harrell_c")

```


## Statistical Analysis

### Global Friedman Test

::: {.panel-tabset}

#### Harrell's C


```{r friedman-harrell-c}
bma_harrell_c$friedman_test(p.adjust.method = "holm") |>
  tablify()
```

#### ISBS

```{r friedman-isbs}
bma_isbs$friedman_test(p.adjust.method = "holm") |>
  tablify()
```

::: 

### Critical Difference Plots: Bonferroni-Dunn

Using Cox (`CPH`) as baseline for comparison, these represent the primary result of the benchmark.

::: {.panel-tabset}

#### Harrell's C

```{r critical-difference-baseline-diff-harrell-c-harrell-c}
#| fig-width: 10
#| fig-height: 6.25

cd_ratio = 10 / 11

plot_bma(
  bma = bma_harrell_c,
  type = "cd_bd",
  measure_id = "harrell_c",
  tuning_measure_id = "harrell_c",
  ratio = cd_ratio,
  baseline = "CPH"
)
```

#### ISBS

```{r critical-difference-baseline-diff-isbs-isbs}
#| fig-width: 10
plot_bma(
  bma = bma_isbs,
  type = "cd_bd",
  measure_id = "isbs",
  tuning_measure_id = "isbs",
  ratio = cd_ratio,
  baseline = "CPH"
)
```

:::
