---
title: "Resource Estimates (December)"
author: "Lukas"
date: now
date-format: "YYYY-MM-DD HH:mm:ss (z)"
format: 
  html:
    code-fold: true
    toc: true
    toc-depth: 3
editor: source
editor_options: 
  chunk_output_type: console
---

```{r setup}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Setup

Benchmark setup for resource estimation:

- Holdout outer resampling (instead of 5-fold CV)
- Holdout inner resampling (instead of 3-fold CV)
- 1 eval random search (instead of 50 per tunable param)

So 2 learner `$train()` and and `$predict()` calls total per tuning measure.
We may or may not have gotten lucky with fast or slow HPCs in the random search eval.
This is just a ballpark estimate.

Changes from last resource estimation:

- Reduces forest learner `num.trees`-like param from 5000 to 1000
- Removed `dcalib` tuning measure entirely
- made `Flex`, `Par`, `RRT` use parameter grid rather than random search due to their small tuning spaces.
- Only run estimation runs on `harrell_c` because times won't be hugely different for `rcll`


```{r}
root = here::here()

cli::cli_alert_info("Loading config \"runtime_est\"")
# settings = config::get(config = "runtime_est")
# source(file.path(root, "settings_runtime_est.R"))

library(batchtools)
library(ggplot2)
library(mlr3batchmark)
library(data.table)

reg_dir = file.path(root, "registry_runtime_est_dec")
reg = loadRegistry(reg_dir, writeable = FALSE)

alljobs = unwrap(getJobTable())[, .(job.id, repl, tags, task_id, learner_id, submitted, time.queued, time.running, mem.used, error)]
# Enhance with task metadata (n, p, number of unique time points etc)
tasktab = read.csv(here::here("attic", "tasktab.csv"))
lrntab = read.csv(here::here("attic", "learners.csv"))
alljobs = ljoin(alljobs, tasktab, by = "task_id")
setkey(alljobs, job.id)

# Estimating runtime via `batchtools` builtin `ranger` only on selected variables (avoid time.queued, repl, note that job.id is excluded automatically):
bt_est = alljobs |>
  dplyr::select(job.id, task_id, learner_id, tags) |>
  batchtools::estimateRuntimes()
# print(bt_est)

# Merging estimates runtimes with observed ones, converting to `hms` for readability:
alljobs = alljobs[bt_est$runtimes]
alljobs[, time.running.hms := hms::hms(seconds = as.numeric(runtime, units = "secs"))]

if (!(file.exists("bmr-resource-est-dec.rds"))) {
  tictoc::tic("reducing results")
  bmr = reduceResultsBatchmark(store_backends = TRUE)
  tictoc::toc()
  aggr = bmr$aggregate(measures = msrs(c("time_predict", "time_train", "time_both")), conditions = TRUE)
  aggr[, resample_result := NULL]

  tictoc::tic(msg = "saving aggr")
  saveRDS(aggr, "aggr-resource-est-dec.rds")
  tictoc::toc()
  tictoc::tic(msg = "saving bmr")
  saveRDS(bmr, "bmr-resource-est-dec.rds")
  tictoc::toc()
}
```

```{r helper_funs}
summarize_resources <- function(xdf, by = "task_id", measure = "runtime") {
  checkmate::assert_subset(by, choices = c("task_id", "learner_id", "tags"))
  checkmate::assert_subset(measure, choices = c("runtime", "mem.used"))
  
  xtab <- xdf |>
    dplyr::filter(type == "observed") |>
    dplyr::summarise(
      min = min(.data[[measure]]),
      q25 = quantile(.data[[measure]], prob = 0.25),
      median = median(.data[[measure]]),
      q75 = quantile(.data[[measure]], prob = 0.75),
      max = max(.data[[measure]]),
      n_jobs = dplyr::n(),
      .by = dplyr::all_of(by)
    )
  
  if (measure == "runtime") {
    xtab <- xtab |> 
      dplyr::mutate(dplyr::across(min:max, \(x) hms::hms(seconds = round(x, 1))))
  }
    if (measure == "mem.used") {
    # mem.used shows MB, pretty_bytes expects bytes
    xtab <- xtab |> 
      dplyr::mutate(dplyr::across(min:max, \(x) round(x/1000, 2)))
  }
  
  xtab |>
    dplyr::arrange(dplyr::desc(.data[["q75"]])) |>
    reactable::reactable(striped = TRUE, filterable = TRUE, defaultPageSize = 20)
}

plot_resources <- function(xdf, by = "task_id", measure = "runtime", ...) {
  checkmate::assert_subset(by, choices = c("task_id", "learner_id", "tags"))
  checkmate::assert_subset(measure, choices = c("runtime", "mem.used"))
  
  p <- xdf |>
    dplyr::filter(..., type == "observed") |>
    ggplot(aes(
      y = reorder(.data[[by]], .data[[measure]]),
      x = .data[[measure]]
    )) +
      geom_boxplot()
  
  if (measure == "runtime") {
    p <- p + scale_x_time(breaks = scales::pretty_breaks())
  }
  if (measure == "mem.used") {
    p <- p + scale_x_continuous(breaks = scales::pretty_breaks(), labels = \(x) prettyunits::pretty_bytes(x * 1000^2))
  }
  
  title <- switch(measure, runtime = "Runtime", mem.used = "Memory")
  x_ax <- switch(measure, runtime = "Runtime (HH:MM:SS)", mem.used = "Memory (GiB)")
  p  +
    labs(
      title = sprintf("Job %s", title),
      subtitle = sprintf("based on %i / %i jobs (%s%%)", sum(alljobs$type == "observed"), nrow(alljobs),
                         round(100 * sum(alljobs$type == "observed")/nrow(alljobs), 1)),
      x = x_ax, y = by
    ) +
    theme_minimal() +
    theme(legend.position = "top")
}
```

### Learners

Special cases for learners:

- `internal_cv` learners like `glmnet` and `CoxBoost` internally use a cross validation. CB is not tuned on the inner folds at all and uses its own method.
- `encode`/`scale` learners need factor encoding or scaling as part of the preprocessing pipeline
- `grid` learners have such small tuning spaces that a grid search is preferable

```{r}
lrntab |>
  dplyr::mutate(dplyr::across(dplyr::where(is.logical), \(x) ifelse(x, "\u2705", ""))) |>
  kableExtra::kbl() |>
  kableExtra::kable_styling()
```

### Tasks

```{r}
tasktab |>
  dplyr::select(task_id, n, p, n_uniq_t) |>
  dplyr::arrange(-n) |>
  kableExtra::kbl() |>
  kableExtra::kable_styling()
```

## Status

### How many jobs are done?

```{r}
alljobs |>
  dplyr::group_by(repl, tags) |>
  dplyr::summarize(
    total = dplyr::n(),
    done = sum(!is.na(time.running)),
    perc = round(100 * done/total, 2)
  ) |>
  kableExtra::kbl() |>
  kableExtra::kable_styling()
```

### Which are still running or expired?

Duration measured in **days** based on "now": `r format(Sys.time(), tz = "Europe/Berlin", format = "%F %T (%Z)")`. If that time is greater than 7 days, they hit the cluster timeout wall.

```{r, eval = nrow(findRunning()) > 0}
alljobs[ijoin(findSubmitted(), findNotDone())]  |>
  dplyr::mutate(duration = round(difftime(Sys.time(), submitted, unit = "days"), 2)) |>
  dplyr::filter(!is.na(duration)) |>
  dplyr::select(-submitted) |>
  reactable::reactable(striped = TRUE, filterable = TRUE)
```


## Runtime 

Only showing observed, not estimated runtimes.

### All Jobs by Runtime
 
```{r}
runtimes <- alljobs |>
  dplyr::filter(type == "observed") |>
  dplyr::select(
    learner_id, task_id,
    n, p, n_uniq_t,
    measure = tags,
    runtime, time.running.hms
  ) |>
  dplyr::arrange(dplyr::desc(runtime))

runtimes |>
  dplyr::mutate(time.running.hms = hms::round_hms(time.running.hms, secs = 1)) |>
  dplyr::select(-runtime) |>
  reactable::reactable(striped = TRUE, filterable = TRUE)
```


### Summarizing by learner / task

Limited to finished jobs only, `n_jobs` shows number of jobs used for summary.

Sorted by 75% quantile, descending.

### Runtime per Task

```{r}
#| column: page

plot_resources(alljobs, by = "task_id", measure = "runtime")
summarize_resources(alljobs, by = "task_id", measure = "runtime")
```

### Runtime per Learner

```{r}
#| column: page

plot_resources(alljobs, by = "learner_id", measure = "runtime")
summarize_resources(alljobs, by = "learner_id", measure = "runtime")
```

### Runtime per Measure

```{r}
#| column: page

plot_resources(alljobs, by = "tags", measure = "runtime")
summarize_resources(alljobs, by = "tags", measure = "runtime")
```

### Train/Predict

```{r}
aggr = readRDS(here::here("aggr-resource-est-dec.rds"))

aggr |>
  dplyr::select(learner_id, task_id, warnings, errors, time_train, time_predict) |>
  dplyr::mutate(dplyr::across(
    dplyr::starts_with("time"), \(x) round(x / 60/60, 1))) |>
  reactable::reactable(striped = TRUE, filterable = TRUE, defaultPageSize = 20)

```


## Memory

Raw values of `mem.used` show MB, converted to GB.

### All jobs by Memory

```{r}
alljobs |>
  dplyr::filter(!is.na(mem.used)) |>
  dplyr::select(
    learner_id, task_id,
    n, p, n_uniq_t,
    measure = tags,
    mem.used
  ) |>
  dplyr::arrange(dplyr::desc(mem.used)) |>
  dplyr::mutate(mem.used = round(mem.used/1000, 3)) |>
  reactable::reactable(striped = TRUE, filterable = TRUE)
```


### Memory per Task

```{r}
#| column: page

plot_resources(alljobs, by = "task_id", measure = "mem.used")
summarize_resources(alljobs, by = "task_id", measure = "mem.used")
```

### Memory per Learner

```{r}
#| column: page

plot_resources(alljobs, by = "learner_id", measure = "mem.used")
summarize_resources(alljobs, by = "learner_id", measure = "mem.used")
```

### Memory per Measure

```{r}
#| column: page

plot_resources(alljobs, by = "tags", measure = "mem.used")
summarize_resources(alljobs, by = "tags", measure = "mem.used")
```

## Total Runtime Estimate

Assumptions:

- Here `runtime` is measured on 1 train/predict on outer and inner holdout splits
  - -> Take `runtime/2` for one train/predict as conservative estimate
  - Most real evals will be on inner folds, i.e. smaller data
- Budget is `tuning_params * 50` evals random search
- Outer folds: 5
- Inner folds: 3
- Some learners are not tuned with inner resampling (KM, CoxBoost (internal CV), ...) 
  -> no params, no tuning
  -> no inner CV in that case

Estimated total runtime is then `runtime * outer_folds * inner_folds * n_params * 50`.

Estimates are partially incomplete as some pretest jobs are not completed yet, hence very long runtimes are missing.

```{r runtime-total-sum}
#| column: page
total_estimate <- runtimes |>
  dplyr::group_by(learner_id, task_id, measure) |>
  dplyr::summarize(
    # seconds per 2x train/predict -> hours per one (average, upper estimate)
    hours = sum(runtime/60/60 / 2), 
    .groups = "drop"
  ) |>
  dplyr::left_join(lrntab, by = "learner_id") |>
  dplyr::select(learner_id, task_id, measure, params, internal_cv, hours) |>
  dplyr::mutate(
    # we tune learners with at least 1 param
    tune = as.integer(params > 0),
    internal_cv = as.integer(internal_cv == "Yes")
  ) |>
  dplyr::mutate(
    n_outer = 5,
    # No inner resampling if no tuning
    n_inner = pmax(3 * tune, 1),
    #n_budget = pmax(50 * params * tune, 1),
    n_budget = dplyr::case_when(
      learner_id == "RRT" ~ 46,
      learner_id == "Par" ~ 3,
      learner_id == "Flex" ~ 10,
      .default = pmax(50 * params * tune, 1)
    ),
    total_per_job = hours * n_inner * n_budget,
    total_h = total_per_job * n_outer,
    total_years = total_h / 24 / 365
  ) |>
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \(x) round(x, 2)))

total_estimate |>
  dplyr::select(-internal_cv, -n_inner, -hours) |>
  dplyr::arrange(-total_h) |>
  reactable::reactable(striped = TRUE, filterable = TRUE)

# total_estimate |>
#   ggplot(aes(x = learner_id, y = total_per_job)) +
#   geom_boxplot() +
#   geom_hline(yintercept = 168, color = "red") +
#   coord_flip()
# 
# total_estimate |>
#   ggplot(aes(x = task_id, y = total_per_job)) +
#   geom_boxplot() +
#   geom_hline(yintercept = 168, color = "red") +
#   coord_flip()
```

### Totals per learner:

```{r}
total_estimate |>
  dplyr::group_by(learner_id) |>
  dplyr::summarise(
    total_h = round(sum(total_h), 2),
    total_years = round(sum(total_years), 2)
  ) |>
  dplyr::arrange(-total_h) |>
  reactable::reactable(striped = TRUE, defaultPageSize = 20)
```

### Totals per task:

```{r}
total_estimate |>
  dplyr::group_by(task_id) |>
  dplyr::summarise(
    total_h = round(sum(total_h), 2),
    total_years = round(sum(total_years), 2)
  ) |>
  dplyr::arrange(-total_h) |>
  reactable::reactable(striped = TRUE, defaultPageSize = 20)
```

### Totals per measure:

```{r}
total_estimate |>
  dplyr::group_by(measure) |>
  dplyr::summarise(
    total_h = round(sum(total_h), 2),
    total_years = round(sum(total_years), 2)
  ) |>
  dplyr::arrange(-total_h) |>
  reactable::reactable(striped = TRUE, defaultPageSize = 20)
```

### Sum total:

```{r}
total_estimate |>
  dplyr::summarise(
    total_h = round(sum(total_h), 2),
    total_years = round(sum(total_years), 2)
  ) |>
  kableExtra::kbl()
```


## Overview

```{r}
memtab = alljobs |>
  dplyr::mutate(mem.used = ifelse(is.na(mem.used), max(mem.used, na.rm = TRUE), mem.used)) |>
  dplyr::group_by(learner_id, task_id, measure = tags) |>
  dplyr::summarise(mem_gb = mean(mem.used/1000), .groups = "drop")

resource_est_tab = total_estimate |>
  dplyr::select(learner_id, task_id, measure, hours, total_h) |>
  dplyr::left_join(memtab, by = c("learner_id", "task_id", "measure"))

resource_est_tab = resource_est_tab |>
  dplyr::mutate(
    hours = ifelse(is.na(hours), max(hours, na.rm = TRUE), hours),
    total_h = ifelse(is.na(total_h), max(total_h, na.rm = TRUE), total_h),
    mem_gb = ifelse(is.na(mem_gb), max(mem_gb, na.rm = TRUE), mem_gb)
  ) 

write.csv(resource_est_tab, file = here::here("attic", "resource_est_dec.csv"), row.names = FALSE)
```

