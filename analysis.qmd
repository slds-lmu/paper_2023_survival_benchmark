---
title: "Survival Benchmark: Analysis"
author: "Lukas"
date: now
date-format: "YYYY-MM-DD HH:mm:ss Z"
format: 
  html:
    code-fold: true
    toc: true
    toc-depth: 5
    embed-resources: false
    fig-align: center
    theme:
      light: flatly
      dark: darkly
lightbox: true
editor: source
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE, dev = "ragg_png")
source(here::here("helpers.R"))
settings = config::get(config = "beartooth")
reg_dir = here::here(settings$reg_name)
result_path = here::here("results")

###################################################################################################
### Packages
###################################################################################################
library(mlr3benchmark)
library(mlr3proba)
requireNamespace("mlr3extralearners")
# requires PMCMRplus, not included in renv because of issues installing it on cluster (libmpfr.so.6)
library(ggplot2)
library(kableExtra)

result_path = here::here("results", settings$reg_name)
msr_tbl = measures_tbl()
measures_eval_ids = msr_tbl$id
lrntab = load_lrntab()
tasktab = load_tasktab()
```

```{r setup-read}
bma_harrell_c = readRDS(fs::path(result_path, "bma_clean_harrell_c.rds"))
bma_rcll      = readRDS(fs::path(result_path, "bma_clean_rcll.rds"))

bmrtab_harrell_c = readRDS(fs::path(result_path, "bmrtab_harrell_c.rds"))
bmrtab_rcll = readRDS(fs::path(result_path, "bmrtab_rcll.rds"))

bma = readRDS(fs::path(result_path, "bma_full.rds"))
```



## Metadata

### Learners

Special cases for learners:

- `internal_cv` learners like `glmnet` and `CoxBoost` internally use a cross validation. CoxB is not tuned on the inner folds at all and uses its own method. 
- `encode`/`scale` learners need factor encoding or scaling as part of the preprocessing pipeline
- `grid` learners have such small tuning spaces that a grid search is preferable

```{r}
lrntab |>
  dplyr::mutate(dplyr::across(dplyr::where(is.logical), \(x) ifelse(x, "\u2705", ""))) |>
  kableExtra::kbl() |>
  kableExtra::kable_styling()
```

### Tasks

```{r}
tasktab |>
  dplyr::select(task_id, n, p, n_uniq_t) |>
  dplyr::arrange(-n) |>
  kableExtra::kbl(col.names = c("Task", "N", "p", "# Unique Time Points")) |>
  kableExtra::kable_styling()
```

### Checking PH Assumption and Administrative Censoring

Using global Schoenfeld test via `survival::coxph()` -> `survival::cox.zph()`.  
`NA`s indicate an error during `cox.zph()`.  

Test is applied to the full data before resampling.

Administrative censoring is defined here as the censoring time being equal to the latest censoring time, and the administrative censoring proportion is the percentage of censored observations that are censored administratively.  
`admin_cens` denotes whether `admin_cens_prop` is greater than half the number of censored observations.

```{r check-ph}
#| fig-width: 8
#| fig-height: 12
#| fig-align: center
#| cache: true

tasks = load_task_data()

# survival::cox.zph(survival::coxph(survival::Surv(time, status) ~ ., data = tasks$CarpenterFdaData))
# survival::cox.zph(survival::coxph(survival::Surv(time, status) ~ ., data = tasks$hdfail))

tab_ph_test_dt = function(tasks, task_id) {
  
  if (length(task_id) > 1) {
    return(data.table::rbindlist(
      lapply(task_id, \(x) tab_ph_test_dt(tasks, x)), 
      use.names = TRUE
    ))
  }
  
  #browser()
  xdf = tasks[[task_id]]
  
  cens_times = xdf[xdf$status == 0, "time"]
  last_cens_time = max(cens_times)
  
  admin_cens = if (sum(cens_times == last_cens_time) > length(cens_times)/2) {
    "Majority"
  } else {
    "Minority"
  }
  admin_cens_prop = round(100 * sum(cens_times == last_cens_time) / length(cens_times), 1)

  cli::cli_alert_info("Checking {task_id}")
  fit_cph = survival::coxph(survival::Surv(time, status) ~ ., data = xdf)
  
  fit_zph = tryCatch(
    survival::cox.zph(fit_cph, terms = FALSE), 
    error = function(e) e
  )
  
  if (inherits(fit_zph, "error")) {
    cli::cli_alert_danger("Error in cox.zph for {task_id}")
    return(data.table(
      task_id = task_id, chisq = 0, df = 0, p = 0, p_fmt = "NA", 
      admin_cens = admin_cens, admin_cens_prop
    ))
  }

  dt = as.data.table(fit_zph$table, keep.rownames = "feature")
  dt = dt[feature == "GLOBAL", ]
  dt[, task_id := ..task_id]
  dt[, admin_cens_prop := admin_cens_prop]
  dt[, admin_cens := admin_cens]
  dt[, feature := NULL]
  dt[, p_fmt := format.pval(p, eps = 0.05)]
  data.table::setcolorder(dt, c("task_id", "chisq", "df", "p", "p_fmt"))
  dt
}
# tab_ph_test_dt(tasks, "flchain")

global_dt = tab_ph_test_dt(tasks, names(tasks))

global_dt |>
  kbl(caption = "Global Schoenfeld Test") |>
  kable_styling() |>
  row_spec(row = which(global_dt$p_fmt == "< 0.05"), bold = TRUE) |>
  row_spec(row = which(global_dt$p_fmt == "NA"), color = "crimson")

```

</details>

#### Performance by PH Violation

Naively assuming p < 0.05 indicates PH violation definitely and using that as a "screening" to categorize tasks and visualize performance:

```{r ph-viol-perf}
#| fig-width: 8
#| fig-height: 7
#| fig-align: center
bma = bma[global_dt[, .(task_id, p, admin_cens)], on = "task_id"][, p := fifelse(p < 0.05, "No PH", "PH")]

# scale_color_manual(values = palette, aesthetics = c("color", "fill"), guide = guide_legend(keywidth = 2)) +

plot_aggr_ph(bma, eval_measure = "harrell_c", tuning_measure = "harrell_c")
plot_aggr_ph(bma, eval_measure = "uno_c", tuning_measure = "harrell_c")


plot_aggr_ph(bma, eval_measure = "brier_improper", tuning_measure = "rcll")
plot_aggr_ph(bma, eval_measure = "brier_proper", tuning_measure = "rcll")
```

Same but also grouped by "is administrative censoring the majority or minority of all censoring"

```{r ph-admincens-perf}
#| fig-width: 8
#| fig-height: 7
#| fig-align: center
bmatemp = bma |>
  dplyr::mutate(p = glue::glue("{p} / {admin_cens} admin. cens."))


plot_aggr_ph(bmatemp, eval_measure = "harrell_c", tuning_measure = "harrell_c")
plot_aggr_ph(bmatemp, eval_measure = "uno_c", tuning_measure = "harrell_c")


plot_aggr_ph(bmatemp, eval_measure = "brier_improper", tuning_measure = "rcll")
plot_aggr_ph(bmatemp, eval_measure = "brier_proper", tuning_measure = "rcll")
```

#### Censoring Dependence

Fitting a logistic regression model on the status indicator (`status ~ .`) and counting the number of significant (`p < 0.05`) coefficients per task.  

- `p` number of features before model
- `n_coefs` number of coefficients in model, *including* intercept and dummy-encoded variables
- `n_signif` number of coefficients with `p < 0.05`
- `signif_prop` = `n_signif / n_coefs` in %

```{r task-censmods}
censmods = data.table::rbindlist(lapply(names(tasks), \(task_id) {
  coef_tbl = glm(status ~ ., data = tasks[[task_id]], family = "binomial") |>
    broom::tidy()
  
  data.frame(
    task_id = task_id,
    p = ncol(tasks[[task_id]]) - 2,
    n_coefs = nrow(coef_tbl),
    n_signif = sum(coef_tbl$p.value < 0.05)
  )
  
}))

censmods |> 
  dplyr::mutate(signif_prop = 100 * round(n_signif / n_coefs, 3)) |>
  dplyr::arrange(-xtfrm(signif_prop)) |>
  tablify()

```


## Completed jobs

All jobs are completed by now.

<details>
<summary>Click to expand tables of job counts</summary>


```{r}
bma_harrell_c$data[, .(n = .N), by = .(learner_id)] |>
  tablify(caption = "Completed jobs by learner")

bma_harrell_c$data[, .(n = .N), by = .(task_id)] |>
  tablify(caption = "Completed jobs by task")
```

</details>

## Counting Errors

### Based on outer resampling folds

::: {.panel-tabset}

#### Harrel's C

```{r sanity-check-harrell}
bmrtab_harrell_c[errors > 0,] |> 
  tablify()

bmrtab_harrell_c[errors > 0,][, .(n = .N), by = .(task_id)] |> 
  tablify()

bmrtab_harrell_c[errors > 0,][, .(n = .N), by = .(learner_id)] |> 
  tablify()
```

#### RCLL

```{r sanity-check-rcll}
bmrtab_rcll[errors > 0,] |> 
  tablify()

bmrtab_rcll[errors > 0,][, .(n = .N), by = .(task_id)] |> 
  tablify()

bmrtab_rcll[errors > 0,][, .(n = .N), by = .(learner_id)] |> 
  tablify()
```


:::

<!-- ### Imputing invalid values in `bma` -->

<!-- By substituting them with fixed bad scores -->

<!-- - Graf Score (Improper) -> 1 (?) -->
<!-- - Graf Score (Improper, ERV) -> 1 (?) -->
<!-- - Alpha -> 10 (same as D-Calibration) -->

<!-- ```{r, eval=FALSE} -->
<!-- check_scores(bma_harrell_c) |> -->
<!--   kbl(caption = "Invalid scores, tuned on Harrell's C") |> -->
<!--   kable_styling() -->

<!-- check_scores(bma_rcll) |> -->
<!--   kbl(caption = "Invalid scores, tuned on RCLL") |> -->
<!--   kable_styling() -->

<!-- bma_harrell_c = truncate_scores(bma_harrell_c, trunc_caliba = 10, trunc_graf_improper = 1, trunc_graf_improper_erv = 1) -->
<!-- bma_rcll      = truncate_scores(bma_rcll,      trunc_caliba = 10, trunc_graf_improper = 1, trunc_graf_improper_erv = 1) -->

<!-- saveRDS(bma_harrell_c, here::here("results/registry_beartooth/bma_clean_harrell_c.rds")) -->
<!-- saveRDS(bma_rcll, here::here("results/registry_beartooth/bma_clean_rcll.rds")) -->
<!-- ``` -->


### Errors from tuning archives

Proportion of errors relative to total number of evaluations, i.e. `nrow()` of the associated tuning archives.

```{r get-archives}
archives = reassemble_archives(settings = settings, keep_logs = FALSE)
archives[, archive_length := vapply(archive, nrow, integer(1))]
#archive_errors = archives[errors_sum > 0, c("tune_measure", "task_id", "learner_id", "errors_sum")]

archives[, .(errors_total = sum(errors_sum), evals_total = sum(archive_length)), by = .(tune_measure, learner_id)] |>
  dplyr::filter(errors_total > 0) |>
  dplyr::select(learner_id, tune_measure, errors_total, evals_total) |>
  dplyr::arrange(learner_id) |>
  dplyr::mutate(error_prop = round(100 * errors_total / evals_total, 1)) |>
  tablify()
```


# Results by Eval Measures

Scores are separated by tuning measure (Harrell's C and RCLL).

### Exclusions

- Learner `SSVM` is excluded due to persistent technical issues that we could not resolve

### Raw scores (`bma`)

::: {.panel-tabset}

#### Harrell's C

```{r}
#| column: page
bma_harrell_c$data |>
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \(x) round(x, 3))) |>
  dplyr::arrange(task_id, learner_id) |>
  reactable::reactable(
    sortable = TRUE, filterable = TRUE, searchable = TRUE
  )
```

#### RCLL

```{r}
#| column: page
bma_rcll$data |>
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \(x) round(x, 3))) |>
  dplyr::arrange(task_id, learner_id) |>
  reactable::reactable(
    sortable = TRUE, filterable = TRUE, searchable = TRUE
  )
```

::: 

### Global Friedman Test

::: {.panel-tabset}

#### Harrell's C


```{r friedman-harrell-c}
bma_harrell_c$friedman_test(p.adjust.method = "holm") |>
  tablify()
```

#### RCLL

```{r friedman-rcll}
bma_rcll$friedman_test(p.adjust.method = "holm") |>
  tablify()
```

::: 

### Mean + SE

::: {.panel-tabset}

#### Harrell's C


```{r meanse-harrell-c}
#| fig-width: 10
for (measure_id in msr_tbl[type == "Discrimination", id]) {
  plot_results(bma = bma_harrell_c, type = "mean", measure_id = measure_id, tuning_measure_id = "harrell_c")
}
```

#### RCLL

```{r meanse-rcll}
#| fig-width: 10
for (measure_id in msr_tbl[type == "Scoring Rule", id]) {
  plot_results(bma = bma_rcll, type = "mean", measure_id = measure_id, tuning_measure_id = "rcll")
}
```

::: 

### Boxplots

::: {.panel-tabset}

#### Harrell's C


```{r boxplot-harrell-c}
#| fig-width: 10
for (measure_id in msr_tbl[type == "Discrimination", id]) {
  plot_results(bma = bma_harrell_c, type = "box", measure_id = measure_id, tuning_measure_id = "harrell_c")
}

```

#### RCLL

```{r boxplot-rcll}
#| fig-width: 10
for (measure_id in msr_tbl[type == "Scoring Rule", id]) {
  plot_results(bma = bma_rcll, type = "box", measure_id = measure_id, tuning_measure_id = "rcll")
}

```

:::

### Friedman-Nemenyi

Not sure if we are actually interested in these plots but they are here for completeness' sake.

<details>
<summary>Click to show plots</summary>


::: {.panel-tabset}


#### Harrell's C

```{r friedman-nemenyi-harrell-c}
#| fig-width: 10
for (measure_id in msr_tbl[type == "Discrimination", id]) {
  plot_results(bma = bma_harrell_c, type = "fn", measure_id = measure_id, tuning_measure_id = "harrell_c")
}
```

#### RCLL

```{r friedman-nemenyi-rcll}
#| fig-width: 10
for (measure_id in msr_tbl[type == "Scoring Rule" & !erv, id]) {
  plot_results(bma = bma_rcll, type = "fn", measure_id = measure_id, tuning_measure_id = "rcll")
}
```

:::

</details>

### Critical Difference Plots: Nemenyi

::: {.panel-tabset}

#### Harrell's C


```{r critical-difference-harrell-c}
#| fig-width: 10
plot_results(bma = bma_harrell_c, type = "cd_n", measure_id = "harrell_c", tuning_measure_id = "harrell_c", ratio = 1)
```

#### RCLL

```{r critical-difference-nemenyi-rcll}
#| fig-width: 10
plot_results(bma = bma_rcll, type = "cd_n", measure_id = "rcll", tuning_measure_id = "rcll", ratio = 1)
```

::: 

### Critical Difference Plots: Bonferroni-Dunn

Using Cox (`CPH`) as baseline.

::: {.panel-tabset}

#### Harrell's C

```{r critical-difference-baseline-diff-harrell-c}
#| fig-width: 10
plot_results(bma = bma_harrell_c, type = "cd_bd", measure_id = "harrell_c", tuning_measure_id = "harrell_c", ratio = 1, baseline = "CPH")
```

#### RCLL

```{r critical-difference-baseline-diff-rcll}
#| fig-width: 10
plot_results(bma = bma_rcll, type = "cd_bd", measure_id = "rcll", tuning_measure_id = "rcll", ratio = 1, baseline = "CPH")
```

::: 


## Calibration

Calculating p-valued for D-Calibration as `pchisq(score, 10 - 1, lower.tail = FALSE)`.

TODO: Not sure how to multiplicity-adjust these p-values, i.e. for a simple Bonferroni-adjustment,
multiply `p` by `n_learners`, `n_tasks`, `n_learners * n_tasks`?
Unclear how we phrase this hypothesis as this is generally more of a "screening" type plot.

```{r}
#| fig-width: 10
#| fig-height: 12
bma[, dcalib_p := pchisq(dcalib, 10 - 1, lower.tail = FALSE)]

for (tuned_on in c("harrell_c", "rcll")) {
  p = ggplot(bma[tuned == tuned_on], aes(y = learner_id, x = dcalib_p, fill = dcalib_p < 0.05)) +
    facet_wrap(vars(task_id), ncol = 8) +
    geom_point(shape = 22) +
    geom_vline(xintercept = 0.05, color = "#E41A1C", linetype = "dotted") +
    scale_x_continuous(
      breaks = c(0.05, 1), limits = c(0, 1)
    ) +
    scale_fill_brewer(palette = "Set1", aesthetics = c("color", "fill")) +
    labs(
      title = "D-Calibration p-values by task and learner",
      subtitle = glue::glue("Models tuned on {msr_tbl[id == tuned_on, label]}"),
      x = "p-value", y = NULL, color = NULL, fill = "p < 0.05 (unadjusted)"
    ) +
    theme_minimal() +
    theme(
      legend.position = "bottom",
      plot.title.position = "plot",
      panel.grid.major.y = element_blank(),
      panel.grid.minor.y = element_blank(),
      panel.spacing.x = unit(5, "mm"),
      panel.background = element_rect(fill = "#EEEEEE", color = "#EEEEEE")
    )
    
    print(p)
}
```


## Comparing Tuning Measures

This is still a very WIP "Not sure how and what to look at specifically here"

```{r}

for (eval_measure in c("harrell_c", "rcll")) {
  p1 = ggplot(bma, aes(x = learner_id, y = .data[[eval_measure]], color = tuned, fill = tuned)) +
    geom_boxplot(alpha = 1/3) +
    scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
    scale_color_brewer(
      palette = "Dark2", 
      labels = \(x) vapply(x, \(y) msr_tbl[id == y, label], ""),
      aesthetics = c("color", "fill")
    ) +
    labs(
      title = "Comparison of aggregated performance depending on tuning measure",
      subtitle = glue::glue("Evaluation Metric: {msr_tbl[id == eval_measure, label]}"),
      x = NULL, y = msr_tbl[id == eval_measure, label],
      color = "Tuning Measure", fill = "Tuning Measure"
    ) +
    theme_minimal() +
    theme(
      plot.title.position = "plot",
      legend.position = "bottom"
    )
  
  p2 = bma |> 
    tidyr::pivot_wider(
      id_cols = c("learner_id", "task_id"), 
      names_from = "tuned", values_from = tidyselect::all_of(eval_measure)
    ) |>
    dplyr::mutate(diff = harrell_c - rcll) |>
    ggplot(aes(y = learner_id, x = diff)) +
    geom_boxplot() +
    labs(
      title = "Difference in performance by tuning measure",
      subtitle = glue::glue("Evaluation Metric: {msr_tbl[id == eval_measure, label]}"),
      caption = "Using performance resulting from tuning on Harrell's C as reference",
      x = NULL, y = glue::glue("Difference in {msr_tbl[id == eval_measure, label]}")
    ) +
    theme_minimal() +
    theme(
      plot.title.position = "plot"
    )
  
  print(p1)
  print(p2)
}

```


<!-- Last chunk to auto-trim CD plots while we're at it -->

```{r trim-critical-diff, eval = requireNamespace("magick", quietly = TRUE)}
# This requires the magick package which is purposefully excluded from renv.lock 
# as it was not easily installable on the cluster due to the libmagick dependency

pngs = fs::dir_ls(knitr::opts_chunk$get()$fig.path, glob = "*critical-diff*png")
purrr::walk(pngs, \(x) {
  magick::image_read(x) |>
    magick::image_trim() |>
    magick::image_write(path = x)
})
```

