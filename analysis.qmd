---
title: "Survival Benchmark: Analysis"
author: "Lukas"
date: now
date-format: "YYYY-MM-DD HH:mm:ss Z"
format: 
  html:
    code-fold: true
    toc: true
    embed-resources: false
    fig-align: center
    theme:
      light: flatly
      dark: darkly
editor: source
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
root = here::here()
source(file.path(root, "helpers.R"))
settings = config::get(config = "beartooth")
reg_dir = file.path(root, settings$reg_name)
result_path = here::here("results")


###################################################################################################
### Packages
###################################################################################################
library(mlr3benchmark)
library(mlr3proba)
requireNamespace("mlr3extralearners")
# requires PMCMRplus, not included in renv because of issues installing it on cluster (libmpfr.so.6)
library(ggplot2)
library(kableExtra)



msr_tbl = measures_tbl()
#measures_eval = msr_tbl$measure
measures_eval_ids = msr_tbl$id
lrntab = load_lrntab()
tasktab = load_tasktab()

plot_results = function(
    bma, type = "box", measure_id = "harrell_c", tuning_measure_id = "harrell_c", ...
    ) {
  
  measure_label = msr_tbl[id == measure_id, label]
  tuning_measure_label = msr_tbl[id == tuning_measure_id, label]
  
  plot_type_label = switch(
    type,
    mean = "Mean Â± SE",
    box = "Boxplot",
    fn = "Post-hoc Friedman-Nemenyi",
    cd_n = "Critical Differences (Nemenyi)",
    cd_bd = "Critical Differences (Baseline KM)"
  )
  
  minimize = msr_tbl[id == measure_id, minimize]
  is_erv = msr_tbl[id == measure_id, erv]

  try({
    
    if (type %in% c("cd_n", "cd_bd") & !is_erv) {
      
      test = switch(type, cd_n = "nemenyi", cd_bd = "bd")
      
      p = mlr3viz::autoplot(bma, type = "cd", meas = measure_id, test = test, minimize = minimize, ...) +
        labs(
          caption = glue::glue("Tuning measure: {tuning_measure_label}
                               Evaluation measure: {measure_label}")
        )
    } else {
      p = mlr3viz::autoplot(bma, type = type, meas = measure_id, ...)
      
      p = p +
        labs(
          title = measure_label,
          subtitle = if (type != "box") plot_type_label else NULL,
          x = NULL,
          y = measure_label,
          caption = glue::glue("Tuning measure: {tuning_measure_label}")
        )
    }
  
    if (type %in% c("box", "mean")) {
      p = p + scale_x_discrete(guide = guide_axis(n.dodge = 2))
      p = p + theme_minimal(base_size = 15)
    }
    
    if (type == "fn") {
      p = p + theme(
        axis.text.x = element_text(angle = 90),
        axis.text.y = element_text(angle = 0)
      )
    }
    
    print(p)
  }, silent = TRUE)
}

tablify = function(x, caption = NULL) {
  x |>
    kableExtra::kbl(caption = caption) |>
    kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
}

```

```{r setup-read}
# bmr_harrell_c  = readRDS(fs::path(result_path, settings$reg_name, "bmr_harrell_c.rds"))

bma_harrell_c  = readRDS(fs::path(result_path, settings$reg_name, "bma_harrell_c.rds"))
bma_rcll       = readRDS(fs::path(result_path, settings$reg_name, "bma_rcll.rds"))

bmrtab_harrell_c = readRDS(fs::path(result_path, settings$reg_name, "bmrtab_harrell_c.rds"))
bmrtab_rcll      = readRDS(fs::path(result_path, settings$reg_name, "bmrtab_rcll.rds"))

# Excluding SSVM results as there are no usable ones
bma_harrell_c = remove_results(bma_harrell_c, learner_id_exclude = "SSVM")
bma_rcll      = remove_results(bma_rcll,      learner_id_exclude = "SSVM")

bma_harrell_c = rename_learners(bma_harrell_c)
bma_rcll      = rename_learners(bma_rcll)

bma1 = bma_harrell_c$data
bma1[, tuned := "harrell_c"]

bma2 = bma_rcll$data
bma2[, tuned := "rcll"]

bma_full = rbind(bma1, bma2)
rm(bma1, bma2)

```

## Metadata

### Learners

Special cases for learners:

- `internal_cv` learners like `glmnet` and `CoxBoost` internally use a cross validation. CB is not tuned on the inner folds at all and uses its own method.
- `encode`/`scale` learners need factor encoding or scaling as part of the preprocessing pipeline
- `grid` learners have such small tuning spaces that a grid search is preferable

```{r}
lrntab |>
  dplyr::mutate(dplyr::across(dplyr::where(is.logical), \(x) ifelse(x, "\u2705", ""))) |>
  kableExtra::kbl() |>
  kableExtra::kable_styling()
```

### Tasks

```{r}
tasktab |>
  dplyr::select(task_id, n, p, n_uniq_t) |>
  dplyr::arrange(-n) |>
  kableExtra::kbl(col.names = c("Task", "N", "p", "# Unique Time Points")) |>
  kableExtra::kable_styling()
```

#### Checking PH Assumptions

Using global Schoenfeld test via `survival::coxph()` -> `survival::cox.zph()`.  
`NA`s indicate an error during `cox.zph()`.

```{r check-ph}
#| fig-width: 8
#| fig-height: 12
#| fig-align: center

tasks = load_task_data()

# survival::cox.zph(survival::coxph(survival::Surv(time, status) ~ ., data = tasks$CarpenterFdaData))
# survival::cox.zph(survival::coxph(survival::Surv(time, status) ~ ., data = tasks$hdfail))

tab_ph_test_dt = function(tasks, task_id) {
  
  if (length(task_id) > 1) {
    return(data.table::rbindlist(lapply(task_id, \(x) tab_ph_test_dt(tasks, x))))
  }
  
  cli::cli_alert_info("Checking {task_id}")
  fit_cph = survival::coxph(survival::Surv(time, status) ~ ., data = tasks[[task_id]])
  
  fit_zph = tryCatch(
    survival::cox.zph(fit_cph, terms = FALSE), 
    error = function(e) e
  )
  
  if (inherits(fit_zph, "error")) {
    cli::cli_alert_danger("Error in cox.zph for {task_id}")
    return(data.table(
      task = task_id, chisq = NA_real_, df = NA_real_, p = NA_real_
    ))
  }
  
  dt = as.data.table(fit_zph$table, keep.rownames = "feature")
  dt = dt[feature == "GLOBAL", ]
  dt[, task := ..task_id]
  dt[, feature := NULL]
  dt[, p := format.pval(p, eps = 0.05)]
  data.table::setcolorder(dt, c("task", "chisq", "df", "p"))
  dt
}

global_dt = tab_ph_test_dt(tasks, names(tasks))

global_dt |>
  kbl(caption = "Global Schoenfeld Test") |>
  kable_styling() |>
  row_spec(row = which(global_dt$p == "< 0.05"), bold = TRUE) |>
  row_spec(row = which(global_dt$p != "< 0.05"), color = "gray") |>
  row_spec(row = which(is.na(global_dt$p)), color = "crimson")

# tab_ph_test = function(tasks, task_id) {
#   cli::cli_alert_info("Checking {task_id}")
#   fit_cph = survival::coxph(survival::Surv(time, status) ~ ., data = tasks[[task_id]])
#   
#   fit_zph = tryCatch(
#     survival::cox.zph(fit_cph, terms = FALSE), 
#     error = function(e) e
#   )
#   
#   if (inherits(fit_zph, "error")) {
#     cli::cli_alert_danger("Error in cox.zph for {task_id}")
#     return(NULL)
#   }
#   
#   testdt = as.data.table(fit_zph$table, keep.rownames = "feature")
#   
#   no_violation = testdt[feature == "GLOBAL", p] >= 0.05
#   if (no_violation) {
#     return("\nSchoenfeld test globally insignificant (p >= 0.05)\n\n")
#   }
#   
#   print(survminer::ggcoxzph(fit_zph))
# 
#   testdt |>
#     kbl(caption = task_id) |>
#     kable_styling() |>
#     row_spec(row = which(testdt$p < 0.05), bold = TRUE)
# }
# 
# for (tid in names(tasks)) {
#   mlr3misc::catf("##### %s \n\n", tid)
#   cat(tab_ph_test(tasks, tid))
# }

```

</details>

## Completed jobs

```{r}
bma_harrell_c$data[, .(n = .N), by = .(learner_id)] |>
  tablify(caption = "Completed jobs by learner")

bma_harrell_c$data[, .(n = .N), by = .(task_id)] |>
  tablify(caption = "Completed jobs by task")
```


## Counting Errors

### Based on outer resampling folds

```{r sanity-check}
# bmrtab_harrell_c |> 
#   knitr::kable() |> 
#   kableExtra::kable_styling()
 
bmrtab_harrell_c[errors > 0,] |> 
  tablify()

bmrtab_harrell_c[errors > 0,][, .(n = .N), by = .(task_id)] |> 
  tablify()

bmrtab_harrell_c[errors > 0,][, .(n = .N), by = .(learner_id)] |> 
  tablify()

```

### Imputing invalid values in `bma`

By substituting them with fixed bad scores

- Graf Score (Improper) -> 1 (?)
- Graf Score (Improper, ERV) -> 1 (?)
- Alpha -> 10 (same as D-Calibration)

```{r}
check_scores(bma_harrell_c) |>
  kbl(caption = "Invalid scores, tuned on Harrell's C") |>
  kable_styling()

check_scores(bma_rcll) |>
  kbl(caption = "Invalid scores, tuned on RCLL") |>
  kable_styling()

bma_harrell_c = truncate_scores(bma_harrell_c, trunc_caliba = 10, trunc_graf_improper = 1, trunc_graf_improper_erv = 1)
bma_rcll      = truncate_scores(bma_rcll,      trunc_caliba = 10, trunc_graf_improper = 1, trunc_graf_improper_erv = 1)

saveRDS(bma_harrell_c, here::here("results/registry_beartooth/bma_clean_harrell_c.rds"))
saveRDS(bma_rcll, here::here("results/registry_beartooth/bma_rcll.rds"))
```


### Errors from tuning archives

Proportion of errors relative to total number of evaluations, i.e. `nrow()` of the associated tuning archives.

```{r get-archives}
archives = reassemble_archives(reg_name = settings$reg_name, result_path = result_path, keep_logs = FALSE)
archives[, archive_length := vapply(archive, nrow, 1)]
#archive_errors = archives[errors_sum > 0, c("tune_measure", "task_id", "learner_id", "errors_sum")]

archives[, .(errors_total = sum(errors_sum), evals_total = sum(archive_length)), by = .(tune_measure, learner_id)] |>
  dplyr::filter(errors_total > 0) |>
  dplyr::select(learner_id, tune_measure, errors_total, evals_total) |>
  dplyr::arrange(learner_id) |>
  dplyr::mutate(error_prop = round(100 * errors_total / evals_total, 1)) |>
  tablify()
```


# Eval Measures

Scores are separated by tuning measure (Harrell's C and RCLL).

### Raw scores (`bma`)

::: {.panel-tabset}

#### Harrell's C

```{r}
bma_harrell_c$data |>
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \(x) round(x, 3))) |>
  dplyr::arrange(task_id, learner_id) |>
  reactable::reactable(
    sortable = TRUE, filterable = TRUE, searchable = TRUE
  )
```

#### RCLL

```{r}
bma_rcll$data |>
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \(x) round(x, 3))) |>
  dplyr::arrange(task_id, learner_id) |>
  reactable::reactable(
    sortable = TRUE, filterable = TRUE, searchable = TRUE
  )
```

::: 

### Global Friedman Test

::: {.panel-tabset}

#### Harrell's C


```{r friedman-harrell-c}
bma_harrell_c$friedman_test(p.adjust.method = "holm") |>
  tablify()
```

#### RCLL

```{r friedman-rcll}
bma_rcll$friedman_test(p.adjust.method = "holm") |>
  tablify()
```

::: 

### Mean + SE

::: {.panel-tabset}

#### Harrell's C


```{r meanse-harrell-c}
#| fig-width: 10
for (measure_id in measures_eval_ids) {
  plot_results(bma = bma_harrell_c, type = "mean", measure_id = measure_id, tuning_measure_id = "harrell_c")
}
```

#### RCLL

```{r meanse-rcll}
#| fig-width: 10
for (measure_id in measures_eval_ids) {
  plot_results(bma = bma_rcll, type = "mean", measure_id = measure_id, tuning_measure_id = "rcll")
}
```

::: 

### Boxplots

::: {.panel-tabset}

#### Harrell's C


```{r boxplot-harrell-c}
#| fig-width: 10
for (measure_id in measures_eval_ids) {
  plot_results(bma = bma_harrell_c, type = "box", measure_id = measure_id, tuning_measure_id = "harrell_c")
}

```

#### RCLL

```{r boxplot-rcll}
#| fig-width: 10
for (measure_id in measures_eval_ids) {
  plot_results(bma = bma_rcll, type = "box", measure_id = measure_id, tuning_measure_id = "rcll")
}

```

:::

### Friedman-Nemenyi

::: {.panel-tabset}


#### Harrell's C

```{r friedman-nemenyi-harrell-c}
#| fig-width: 10
for (measure_id in measures_eval_ids) {
  if (measure_id %in% c("caliba")) next
  plot_results(bma = bma_harrell_c, type = "fn", measure_id = measure_id, tuning_measure_id = "harrell_c")
}
```

#### RCLL

```{r friedman-nemenyi-rcll}
#| fig-width: 10
for (measure_id in measures_eval_ids) {
  if (measure_id %in% c("caliba")) next
  plot_results(bma = bma_rcll, type = "fn", measure_id = measure_id, tuning_measure_id = "rcll")
}
```

:::

### Critical Difference Plots: Nemenyi

::: {.panel-tabset}

#### Harrell's C


```{r critical-difference-harrell-c}
#| fig-width: 10
for (measure_id in measures_eval_ids) {
  if (measure_id %in% c("caliba")) next
  plot_results(bma = bma_harrell_c, type = "cd_n", measure_id = measure_id, tuning_measure_id = "harrell_c", ratio = 1.05)
}
```


#### RCLL

```{r critical-difference-rcll}
#| fig-width: 10
for (measure_id in measures_eval_ids) {
  if (measure_id %in% c("caliba")) next
  plot_results(bma = bma_rcll, type = "cd_n", measure_id = measure_id, tuning_measure_id = "rcll", ratio = 1.05)
}
```

::: 





## Comparing Tning Measures

```{r}
ggplot(bma_full, aes(x = learner_id, y = rcll, color = tuned, fill = tuned)) +
  geom_boxplot(alpha = 1/3) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  scale_color_brewer(palette = "Dark2", aesthetics = c("color", "fill")) +
  theme_minimal() +
  theme(legend.position = "top")
```




<!-- Last chunk to auto-trim CD plots while we're at it -->

```{r trim-critical-diff}
pngs = fs::dir_ls("analysis_files/figure-html/", glob = "*critical-diff*png")
purrr::walk(pngs, \(x) {
  magick::image_read(x) |>
    magick::image_trim() |>
    magick::image_write(path = x)
})
```

