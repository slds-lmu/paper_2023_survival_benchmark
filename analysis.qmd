---
title: "Survival Benchmark: Analysis"
author: "Lukas"
date: now
date-format: "YYYY-MM-DD HH:mm:ss Z"
format: 
  html:
    code-fold: true
    toc: true
    toc-expand: 2
    toc-depth: 5
    embed-resources: false
    fig-align: center
    theme:
      light: flatly
      dark: darkly
lightbox: true
editor: source
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
source(here::here("site/common.R"))
```

## Metadata

### Learners

Special cases for learners:

- `internal_cv` learners like `glmnet` and `CoxBoost` internally use a cross validation. CoxB is not tuned on the inner folds at all and uses its own method. 
- `encode`/`scale` learners need factor encoding or scaling as part of the preprocessing pipeline
- `grid` learners have such small tuning spaces that a grid search is preferable

```{r}
lrntab |>
  dplyr::mutate(dplyr::across(dplyr::where(is.logical), \(x) ifelse(x, "\u2705", ""))) |>
  kableExtra::kbl() |>
  kableExtra::kable_styling()
```

### Tasks

```{r}
# tasktab |>
#   dplyr::select(task_id, n, p, n_uniq_t) |>
#   dplyr::arrange(-n) |>
#   kableExtra::kbl(col.names = c("Task", "N", "p", "# Unique Time Points")) |>
#   kableExtra::kable_styling()

tasktab |>
  dplyr::select(task_id, n, p, events, censprop, n_uniq_t) |>
  dplyr::arrange(-n) |>
  dplyr::mutate(
    n = scales::comma(n),
    p = scales::comma(p),
    events = scales::comma(events),
    censprop = scales::percent(censprop),
    n_uniq_t = scales::comma(n_uniq_t)
  ) |>
  reactable::reactable(sortable = TRUE, filterable = TRUE, pagination = FALSE)

```

### Checking PH Assumption and Administrative Censoring

Using global Schoenfeld test via `survival::coxph()` -> `survival::cox.zph()`.  
`NA`s indicate an error during `cox.zph()`.  

Test is applied to the full data before resampling.

Administrative censoring is defined here as the censoring time being equal to the latest censoring time, and the administrative censoring proportion is the percentage of censored observations that are censored administratively.  
`admin_cens` denotes whether `admin_cens_prop` is greater than half the number of censored observations.

```{r check-ph}
#| fig-width: 8
#| fig-height: 12
#| fig-align: center
#| cache: true

tasks = load_task_data()

# survival::cox.zph(survival::coxph(survival::Surv(time, status) ~ ., data = tasks$CarpenterFdaData))
# survival::cox.zph(survival::coxph(survival::Surv(time, status) ~ ., data = tasks$hdfail))

tab_ph_test_dt = function(tasks, task_id) {
  
  if (length(task_id) > 1) {
    return(data.table::rbindlist(
      lapply(task_id, \(x) tab_ph_test_dt(tasks, x)), 
      use.names = TRUE
    ))
  }
  
  #browser()
  xdf = tasks[[task_id]]
  
  cens_times = xdf[xdf$status == 0, "time"]
  last_cens_time = max(cens_times)
  
  admin_cens = if (sum(cens_times == last_cens_time) > length(cens_times)/2) {
    "Majority"
  } else {
    "Minority"
  }
  admin_cens_prop = round(100 * sum(cens_times == last_cens_time) / length(cens_times), 1)

  cli::cli_alert_info("Checking {task_id}")
  fit_cph = survival::coxph(survival::Surv(time, status) ~ ., data = xdf)
  
  fit_zph = tryCatch(
    survival::cox.zph(fit_cph, terms = FALSE), 
    error = function(e) e
  )
  
  if (inherits(fit_zph, "error")) {
    cli::cli_alert_danger("Error in cox.zph for {task_id}")
    return(data.table(
      task_id = task_id, chisq = 0, df = 0, p = 0, p_fmt = "NA", 
      admin_cens = admin_cens, admin_cens_prop
    ))
  }

  dt = as.data.table(fit_zph$table, keep.rownames = "feature")
  dt = dt[feature == "GLOBAL", ]
  dt[, task_id := ..task_id]
  dt[, admin_cens_prop := admin_cens_prop]
  dt[, admin_cens := admin_cens]
  dt[, feature := NULL]
  dt[, p_fmt := format.pval(p, eps = 0.05)]
  data.table::setcolorder(dt, c("task_id", "chisq", "df", "p", "p_fmt"))
  dt
}
# tab_ph_test_dt(tasks, "flchain")

global_dt = tab_ph_test_dt(tasks, names(tasks))

global_dt |>
  kbl(caption = "Global Schoenfeld Test") |>
  kable_styling() |>
  row_spec(row = which(global_dt$p_fmt == "< 0.05"), bold = TRUE) |>
  row_spec(row = which(global_dt$p_fmt == "NA"), color = "crimson")

```

</details>

#### Performance by PH Violation

Naively assuming p < 0.05 indicates definitive PH violation and using that as a "screening" to categorize tasks and visualize performance:

```{r ph-viol-perf}
#| fig-width: 8
#| fig-height: 7
#| fig-align: center
#| cache: true

bma = bma[global_dt[, .(task_id, p, admin_cens)], on = "task_id"][, p := fifelse(p < 0.05, "No PH", "PH")]
# scale_color_manual(values = palette, aesthetics = c("color", "fill"), guide = guide_legend(keywidth = 2)) +
plot_aggr_ph(bma, eval_measure = "harrell_c", tuning_measure = "harrell_c")
plot_aggr_ph(bma, eval_measure = "uno_c", tuning_measure = "harrell_c")

plot_aggr_ph(bma, eval_measure = "brier_improper", tuning_measure = "rcll")
plot_aggr_ph(bma, eval_measure = "brier_proper", tuning_measure = "rcll")
```

Same but also grouped by "is administrative censoring the majority or minority of all censoring"

```{r ph-admincens-perf}
#| fig-width: 8
#| fig-height: 7
#| fig-align: center
#| cache: true

bmatemp = bma |>
  dplyr::mutate(p = glue::glue("{p} / {admin_cens} admin. cens."))

plot_aggr_ph(bmatemp, eval_measure = "harrell_c", tuning_measure = "harrell_c")
plot_aggr_ph(bmatemp, eval_measure = "uno_c", tuning_measure = "harrell_c")

plot_aggr_ph(bmatemp, eval_measure = "brier_improper", tuning_measure = "rcll")
plot_aggr_ph(bmatemp, eval_measure = "brier_proper", tuning_measure = "rcll")
```

#### Censoring Dependence

Fitting a logistic regression model on the status indicator (`status ~ .`) and counting the number of significant (`p < 0.05`) coefficients per task.  

- `p` number of features before model
- `n_coefs` number of coefficients in model, *excluding* intercept and `time`, *including* and dummy-encoded variables
- `n_signif` number of coefficients with `p < 0.05`
- `signif_prop` = `n_signif / n_coefs` in %

```{r task-censmods}
#| cache: true
#| 
censmods = data.table::rbindlist(lapply(names(tasks), \(task_id) {
  taskdat = tasks[[task_id]]
  taskdat = taskdat[, names(taskdat) != "time"]
  
  coef_tbl = glm(status ~ ., data = taskdat, family = "binomial") |>
    broom::tidy() |>
    dplyr::filter(term != "(Intercept)")
  
  data.frame(
    task_id = task_id,
    p = ncol(tasks[[task_id]]) - 2,
    n_coefs = nrow(coef_tbl),
    n_signif = sum(coef_tbl$p.value < 0.05)
  )
  
}))

censmods |> 
  dplyr::mutate(signif_prop = 100 * round(n_signif / n_coefs, 3)) |>
  dplyr::arrange(-xtfrm(signif_prop)) |>
  tablify()
```


<!-- ## Completed jobs -->

<!-- All jobs are completed by now. -->

<!-- <details> -->
<!-- <summary>Click to expand tables of job counts</summary> -->


<!-- ```{r} -->
<!-- bma_harrell_c$data[, .(n = .N), by = .(learner_id)] |> -->
<!--   tablify(caption = "Completed jobs by learner") -->

<!-- bma_harrell_c$data[, .(n = .N), by = .(task_id)] |> -->
<!--   tablify(caption = "Completed jobs by task") -->
<!-- ``` -->

<!-- </details> -->

## Counting Errors

### Based on outer resampling folds

::: {.panel-tabset}

#### Harrel's C

```{r sanity-check-harrell}
bmrtab_harrell_c[errors > 0,] |> 
  tablify()

bmrtab_harrell_c[errors > 0,][, .(n = .N), by = .(task_id)] |> 
  tablify()

bmrtab_harrell_c[errors > 0,][, .(n = .N), by = .(learner_id)] |> 
  tablify()
```

#### RCLL

```{r sanity-check-rcll}
bmrtab_rcll[errors > 0,] |> 
  tablify()

bmrtab_rcll[errors > 0,][, .(n = .N), by = .(task_id)] |> 
  tablify()

bmrtab_rcll[errors > 0,][, .(n = .N), by = .(learner_id)] |> 
  tablify()
```


:::

### Errors from tuning archives

Proportion of errors relative to total number of evaluations, i.e. `nrow()` of the associated tuning archives.

```{r get-archives}
#| cache: true
archives = reassemble_archives(settings = settings, keep_logs = FALSE)
archives[, archive_length := vapply(archive, nrow, integer(1))]
#archive_errors = archives[errors_sum > 0, c("tune_measure", "task_id", "learner_id", "errors_sum")]

archives[, .(errors_total = sum(errors_sum), evals_total = sum(archive_length)), by = .(tune_measure, learner_id)] |>
  dplyr::filter(errors_total > 0) |>
  dplyr::select(learner_id, tune_measure, errors_total, evals_total) |>
  dplyr::arrange(learner_id) |>
  dplyr::mutate(error_prop = round(100 * errors_total / evals_total, 1)) |>
  tablify()
```

## Results by Evaluation Measures

Scores are separated by tuning measure (Harrell's C and RCLL).

### Measures

Here we consider discrimination measures and scoring rules, with calibration measures getting a separate section below.

`[ERV]` denotes *Explained Residual Variation*, which normalises the score with the score of the Kaplan-Meier estimator, i.e. 

$$S_{\mathrm{ERV}} = 1 - \frac{S_{\mathrm{model}}}{S_{\mathrm{KM}}}$$

### Exclusions

- Learner `SSVM` is excluded due to persistent technical issues that we could not resolve

### Raw scores (`bma`)

::: {.panel-tabset}

#### Harrell's C

```{r}
#| column: page
#| 
bma_harrell_c$data |>
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \(x) round(x, 3))) |>
  dplyr::arrange(task_id, learner_id) |>
  reactable::reactable(
    sortable = TRUE, filterable = TRUE, searchable = TRUE, defaultPageSize = 30
  )
```

#### RCLL

```{r}
#| column: page
bma_rcll$data |>
  dplyr::mutate(dplyr::across(dplyr::where(is.numeric), \(x) round(x, 3))) |>
  dplyr::arrange(task_id, learner_id) |>
  reactable::reactable(
    sortable = TRUE, filterable = TRUE, searchable = TRUE, defaultPageSize = 30
  )
```

::: 

### Global Friedman Test

::: {.panel-tabset}

#### Harrell's C


```{r friedman-harrell-c}
bma_harrell_c$friedman_test(p.adjust.method = "holm") |>
  tablify()
```

#### RCLL

```{r friedman-rcll}
bma_rcll$friedman_test(p.adjust.method = "holm") |>
  tablify()
```

::: 

### Mean + SE

::: {.panel-tabset}

#### Harrell's C


```{r meanse-harrell-c}
#| fig-width: 10
#| fig-height: 8
for (measure_id in msr_tbl[type == "Discrimination" & !erv, id]) {
  plot_results(bma = bma_harrell_c, type = "mean", measure_id = measure_id, tuning_measure_id = "harrell_c", flip = TRUE, dodge = FALSE)
}
  
plot_results(bma = bma_harrell_c, type = "mean", measure_id = "brier_improper", tuning_measure_id = "harrell_c", flip = TRUE, dodge = FALSE)
```

```{r meanse-harrell-c-erv}
#| fig-width: 10
#| fig-height: 8
for (measure_id in msr_tbl[type == "Discrimination" & erv, id]) {
  plot_results(bma = bma_harrell_c, type = "mean", measure_id = measure_id, tuning_measure_id = "harrell_c", flip = TRUE, dodge = FALSE)
}
  
plot_results(bma = bma_harrell_c, type = "mean", measure_id = "brier_improper", tuning_measure_id = "harrell_c", flip = TRUE, dodge = FALSE)
```

#### RCLL

```{r meanse-rcll}
#| fig-width: 10
#| fig-height: 8
for (measure_id in msr_tbl[type == "Scoring Rule" & !erv, id]) {
  plot_results(bma = bma_rcll, type = "mean", measure_id = measure_id, tuning_measure_id = "rcll", flip = TRUE, dodge = FALSE)
}
```

```{r meanse-rcll-erv}
#| fig-width: 10
#| fig-height: 8
for (measure_id in msr_tbl[type == "Scoring Rule" & erv, id]) {
  plot_results(bma = bma_rcll, type = "mean", measure_id = measure_id, tuning_measure_id = "rcll", flip = TRUE, dodge = FALSE)
}
```

::: 

### Boxplots

::: {.panel-tabset}

#### Harrell's C

```{r boxplot-harrell-c}
#| fig-width: 11
#| fig-height: 8
for (measure_id in msr_tbl[type == "Discrimination" & !erv, id]) {
  plot_results(bma = bma_harrell_c, type = "box", measure_id = measure_id, tuning_measure_id = "harrell_c", dodge = FALSE, flip = TRUE)
}

# Added Harrell's C -> ISBS last minute
plot_results(bma = bma_harrell_c, type = "box", measure_id = "brier_improper", tuning_measure_id = "harrell_c", dodge = FALSE, flip = TRUE)

```

```{r boxplot-harrell-c-erv}
#| fig-width: 11
#| fig-height: 8
for (measure_id in msr_tbl[type == "Discrimination" & erv, id]) {
  plot_results(bma = bma_harrell_c, type = "box", measure_id = measure_id, tuning_measure_id = "harrell_c", dodge = FALSE, flip = TRUE)
}
```

#### RCLL

```{r boxplot-rcll}
#| fig-width: 11
#| fig-height: 8
for (measure_id in msr_tbl[type == "Scoring Rule" & !erv, id]) {
  plot_results(bma = bma_rcll, type = "box", measure_id = measure_id, tuning_measure_id = "rcll", dodge = FALSE, flip = TRUE)
}
```

```{r boxplot-rcll-erv}
#| fig-width: 11
#| fig-height: 8
for (measure_id in msr_tbl[type == "Scoring Rule" & erv, id]) {
  plot_results(bma = bma_rcll, type = "box", measure_id = measure_id, tuning_measure_id = "rcll", dodge = FALSE, flip = TRUE)
}
```

:::

<!-- ### Friedman-Nemenyi -->

<!-- Not sure if we are actually interested in these plots but they are here for completeness' sake. -->

<!-- <details> -->
<!-- <summary>Click to show plots</summary> -->


<!-- ::: {.panel-tabset} -->


<!-- #### Harrell's C -->

<!-- ```{r friedman-nemenyi-harrell-c} -->
<!-- #| fig-width: 10 -->
<!-- for (measure_id in msr_tbl[type == "Discrimination", id]) { -->
<!--   plot_results(bma = bma_harrell_c, type = "fn", measure_id = measure_id, tuning_measure_id = "harrell_c") -->
<!-- } -->
<!-- ``` -->

<!-- #### RCLL -->

<!-- ```{r friedman-nemenyi-rcll} -->
<!-- #| fig-width: 10 -->
<!-- for (measure_id in msr_tbl[type == "Scoring Rule" & !erv, id]) { -->
<!--   plot_results(bma = bma_rcll, type = "fn", measure_id = measure_id, tuning_measure_id = "rcll") -->
<!-- } -->
<!-- ``` -->

<!-- ::: -->

<!-- </details> -->

### Critical Difference Plots: Bonferroni-Dunn

Using Cox (`CPH`) as baseline for comparison, these represent the primary result of the benchmark.

::: {.panel-tabset}

#### Harrell's C

```{r critical-difference-baseline-diff-harrell-c-harrell-c}
#| fig-width: 10

cd_ratio = 10/12

plot_results(bma = bma_harrell_c, type = "cd_bd", measure_id = "harrell_c", tuning_measure_id = "harrell_c", ratio = cd_ratio, baseline = "CPH")
```

```{r critical-difference-baseline-diff-harrell-c-isbs}
#| fig-width: 10

cd_ratio = 10/12

plot_results(bma = bma_harrell_c, type = "cd_bd", measure_id = "brier_improper", tuning_measure_id = "harrell_c", ratio = cd_ratio, baseline = "CPH")
```


#### RCLL

```{r critical-difference-baseline-diff-rcll-rcll}
#| fig-width: 10
plot_results(bma = bma_rcll, type = "cd_bd", measure_id = "rcll", tuning_measure_id = "rcll", ratio = cd_ratio, baseline = "CPH")
```


```{r critical-difference-baseline-diff-rcll-isbs}
#| fig-width: 10
plot_results(bma = bma_rcll, type = "cd_bd", measure_id = "brier_improper", tuning_measure_id = "rcll", ratio = cd_ratio, baseline = "CPH")
```


:::

### Critical Difference Plots: Nemenyi

These are not the primary results as they test for all pairwise comparisons, they are merely here for informative purposes.

::: {.panel-tabset}

#### Harrell's C


```{r critical-difference-nemenyi-harrell-c}
#| fig-width: 10
plot_results(bma = bma_harrell_c, type = "cd_n", measure_id = "harrell_c", tuning_measure_id = "harrell_c", ratio = 1)
```

#### RCLL

```{r critical-difference-nemenyi-rcll}
#| fig-width: 10
plot_results(bma = bma_rcll, type = "cd_n", measure_id = "rcll", tuning_measure_id = "rcll", ratio = 1)

plot_results(bma = bma_rcll, type = "cd_n", measure_id = "brier_improper", tuning_measure_id = "rcll", ratio = 1)
```

::: 

## D-Calibration

Calculating p-valued for D-Calibration as `pchisq(score, 10 - 1, lower.tail = FALSE)`.

This represents more of a heuristic approach as an insignificant result implies a well-calibrated model, but a significant result does not necessarily imply a poorly calibrated model.
Furthermore, there is no multiplicity correction applied due to the generally exploratory nature of the plots.

```{r dcalibration-heatmap, fig.width=10, fig.height=8}

bma[, dcalib_p := pchisq(dcalib, 10 - 1, lower.tail = FALSE)]
bma[, dcalib_label := fifelse(dcalib_p < 0.05, "X", "")]

for (tuned_on in c("harrell_c", "rcll")) {
  p = ggplot(bma[tuned == tuned_on], 
        aes(x = forcats::fct_reorder(learner_id, dcalib_p), 
          y = forcats::fct_rev(task_id), 
          fill = dcalib_p)) +
    geom_tile(color = "#EEEEEE") +
    geom_text(aes(label = dcalib_label), color = "white", size = 3) +
    # scale_fill_manual(values = c(`TRUE` = "red", `FALSE` = "blue"), labels = c(`TRUE` = "Signif.", `FALSE` = "Not Signif.")) +
    scale_fill_viridis_c(breaks = seq(0, 1, .1)) +
    guides(
      x = guide_axis(n.dodge = 2), 
      fill = guide_colorbar(
        title.vjust = .8,
        barwidth = unit(200, "pt")
    )) +
    labs(
      title = "D-Calibration p-values by task and learner",
      subtitle = glue::glue(
        "Models tuned on {msr_tbl[id == tuned_on, label]}\n",
        "Learners ordered by average p-value. X denotes p < 0.05"
      ),
      y = "Task", x = "Learner", color = NULL, fill = "p-value"
    ) +
    theme_minimal() +
    theme(
      legend.position = "bottom",
      plot.title.position = "plot",
      panel.grid.major.y = element_blank(),
      panel.grid.minor.y = element_blank(),
      panel.spacing.x = unit(5, "mm"),
      panel.background = element_rect(fill = "#EEEEEE", color = "#EEEEEE")
    )

  print(p)

}

```

## Alpha-Calibration

```{r calib-alpha-ratio-plot}

for (tuned_on in c("harrell_c", "rcll")) {
  p = ggplot(bma[tuned == tuned_on], aes(y = forcats::fct_rev(learner_id), x = caliba_ratio)) +
    geom_point() +
    geom_vline(xintercept = 1) +
    scale_x_log10() +
    labs(
      title = "Alpha-Calibration by task and learner",
      subtitle = glue::glue(
        "Models tuned on {msr_tbl[id == tuned_on, label]}\n",
        "Values close to 1 indicate reasonable calibration"
      ),
      y = "Learner", x = "Alpha (log10)"
    ) +
    theme_minimal() +
    theme(
      legend.position = "bottom",
      plot.title.position = "plot",
      panel.grid.major.y = element_blank(),
      panel.grid.minor.y = element_blank()
      # panel.spacing.x = unit(5, "mm"),
      # panel.background = element_rect(fill = "#EEEEEE", color = "#EEEEEE")
    )

  print(p)

}
```


## Calibration Table

```{r calibration-table}
#| eval: false

for (tuned_on in c("harrell_c", "rcll")) {
  result_tab = bma[tuned == tuned_on, .(learner_id, task_id, dcalib, dcalib_p, caliba_ratio, caliba_diff)]

  new_names = vapply(names(result_tab), \(x) {
                if (!(x %in% msr_tbl$id)) return(x) else msr_tbl[id == x, label]
              }, FUN.VALUE = "", USE.NAMES = FALSE)
  names(result_tab) = new_names

  groupings = table(result_tab$task_id)

  result_tab |>
   mutate(across(where(is.numeric), \(x) round(x, 3))) |>
   #arrange(learner_id, task_id) |>
   select(-task_id) |>
   kableExtra::kbl(booktabs = TRUE, col.names = c(learner_id = "")) |>
   kableExtra::kable_styling() |>
   kableExtra::group_rows(group_label = names(groupings), index = groupings) |>
   print()
}

```

## Comparing Tuning Measures

This is still a very WIP "Not sure how and what to look at specifically here"

```{r tune-measure-comp}
#| cache: true

for (eval_measure in c("harrell_c", "rcll")) {
  p1 = ggplot(bma, aes(x = learner_id, y = .data[[eval_measure]], color = tuned, fill = tuned)) +
  geom_boxplot(alpha = 1/3) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  scale_color_brewer(
    palette = "Dark2", 
    labels = function(x) {
      vapply(x, function(y) {msr_tbl[id == y, label]}, FUN.VALUE = character(1))
    },
    aesthetics = c("color", "fill")
  ) +
  labs(
    title = "Comparison of aggregated performance depending on tuning measure",
    subtitle = glue::glue("Evaluation Metric: {msr_tbl[id == eval_measure, label]}"),
    x = NULL, y = msr_tbl[id == eval_measure, label],
    color = "Tuning Measure", fill = "Tuning Measure"
  ) +
  theme_minimal() +
  theme(
    plot.title.position = "plot",
    legend.position = "bottom"
  )
  
  p2 = bma |>
    tidyr::pivot_wider(
      id_cols = c("learner_id", "task_id"),
      names_from = "tuned", values_from = tidyselect::all_of(eval_measure)
    ) |>
    dplyr::mutate(diff = harrell_c - rcll) |>
    ggplot(aes(y = learner_id, x = diff)) +
    geom_boxplot() +
    labs(
      title = "Difference in performance by tuning measure",
      subtitle = glue::glue("Evaluation Metric: {msr_tbl[id == eval_measure, label]}"),
      caption = "Using performance resulting from tuning on Harrell's C as reference",
      x = NULL, y = glue::glue("Difference in {msr_tbl[id == eval_measure, label]}")
    ) +
    theme_minimal() +
    theme(
      plot.title.position = "plot"
    )

  print(p1)
  print(p2)
}
```

Complete aggregated results table

```{r aggr-table}


tab_input = bma |>
  select(
    "task_id", "learner_id", "harrell_c", "uno_c", "rcll", "rcll_erv", 
    "logloss", "logloss_erv", "intlogloss", "intlogloss_erv", "brier_proper", 
    "brier_proper_erv", "brier_improper", "brier_improper_erv", "dcalib", 
    "caliba_ratio", "caliba_diff", "tuned"
  ) |>
  tidyr::pivot_longer(cols = harrell_c:caliba_diff, names_to = "measure", values_to = "score") |>
  summarize(score = mean(score), .by = c("learner_id", "tuned", "measure")) |>
  mutate(score = round(score, 3)) |>
  mutate(
    measure = vapply(measure, \(x) {msr_tbl[id == x, label]}, FUN.VALUE = "", USE.NAMES = FALSE),
    tuned = vapply(tuned, \(x) {msr_tbl[id == x, label]}, FUN.VALUE = "", USE.NAMES = FALSE)
  ) |>
  tidyr::pivot_wider(id_cols = c("tuned", "measure"), values_from = "score", names_from = "learner_id")

groupings = table(tab_input$tuned)

# HTML version here
tab_input |>
  select(-tuned, ` ` = measure) |>
  kableExtra::kbl() |>
  kableExtra::kable_styling() |>
  kableExtra::group_rows(group_label = names(groupings), index = groupings)

# latex version for saving
# tab_input |>
#   select(-tuned, ` ` = measure) |>
#   kableExtra::kbl(booktabs = TRUE, format = "latex") |>
#   kableExtra::kable_styling() |>
#   kableExtra::group_rows(group_label = names(groupings), index = groupings) |>
#   kableExtra::landscape() |>
#   writeLines(con = fs::path(settings$result_path, "aggr_table", ext = "tex"))

```



<!-- Last chunk to auto-trim CD plots while we're at it -->

```{r trim-critical-diff}
if (!requireNamespace("magick", quietly = TRUE)) {
  warning("Needs magick package to auto-trim CD plots")
} else {
  # This requires the magick package which is purposefully excluded from renv.lock 
  # as it was not easily installable on the cluster due to the libmagick dependency
  
  pngs = fs::dir_ls(knitr::opts_chunk$get()$fig.path, glob = "*critical-diff*png")
  
  purrr::walk(pngs, \(x) {
    magick::image_read(x) |>
      magick::image_trim() |>
      magick::image_write(path = x)
  })
}

```
